{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc77a47",
   "metadata": {},
   "source": [
    "## Review/Questions [10 minutes]\n",
    "1. See Nov15 TUT and Nov18 LEC\n",
    "    1. The *confusion matrix*  is the most relevant topic to review for the upcoming week’s material\n",
    "    \n",
    "> If anything is unclear about the concepts we've covered so far, now is a good opportunity to raise your questions before we dive into the ethics module!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca4da2",
   "metadata": {},
   "source": [
    "## Demo and Discussion [50 minutes]: *Biases and Unfairness*? <br>Clinical Trials and the 'Adult Income' Dataset\n",
    "\n",
    "### 1. Clinical Trial: Confusion Matrix and Ethical Considerations [25/50 minutes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41f639",
   "metadata": {},
   "source": [
    "> Lyme disease is a tick-borne illness that can cause severe health problems if not treated early. Early symptoms are often nonspecific and can mimic less severe viral illnesses, making accurate diagnosis necessary. A healthtech company has developed a predictive machine learning model intended to assist doctors in diagnosing Lyme disease based on symptoms, history of tick exposure, and initial blood tests. The model was tested in a clinical trial involving patients suspected of having Lyme disease based on their symptoms and exposure history. \n",
    "\n",
    "#### A. *Confusion matrix* and *metrics* \n",
    "\n",
    "|                 | Predict Disease | Predict No Disease |\n",
    "|-----------------|-----------------|--------------------|\n",
    "| **Disease**     | 23 (TP)         | 7 (FN)             |\n",
    "| **No Disease**  | 90 (FP)         | 280 (TN)           |\n",
    "\n",
    "- **True Positives (TP)**: 23 - The model correctly predicted the disease.\n",
    "- **False Negatives (FN)**: 7 - The model incorrectly predicted no disease when the disease was present.\n",
    "- **False Positives (FP)**: 90 - The model incorrectly predicted the disease when it was absent.\n",
    "- **True Negatives (TN)**: 280 - The model correctly predicted no disease.\n",
    "\n",
    "**Accuracy**: $\\frac{TP + TN}{TP + FN + FP + TN} = \\frac{23 + 280}{23 + 7 + 90 + 280} = 0.759$\n",
    "\n",
    "**Specificity**: $\\frac{TN}{FP + TN} = \\frac{280}{90 + 280} = 0.757$\n",
    "\n",
    "**Sensitivity** : $\\frac{TP}{TP + FN} = \\frac{23}{23 + 7} = 0.767$\n",
    "\n",
    "\n",
    "> The following section now explores the consideration that the above *metrics* are measured with respect to the \"truth\" (which we don't know), as opposed to the prediction that we make... perhaps considering a \"different denominator\" that looks at things from a different perspective is relevant here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf38e86",
   "metadata": {},
   "source": [
    "#### B. *Class Imbalance and Ethical Considerations* for relevant decision-making perspectives\n",
    "\n",
    "1. *Healthcare Provider Decision - Treatment Commencement*\n",
    "    1. **Context**: A provider reviews the model output before deciding to treat a patient suspected of Lyme disease.\n",
    "    2. **Question**: Should the healthcare provider initiate treatment based solely on the model’s diagnosis considering the high rate of false positives, or should additional tests be done first? What are the ethical implications of either decision, especially given the risk of progression in untreated Lyme disease?\n",
    "\n",
    "\n",
    "2. *Patient Perspective - Trust in Medical Advice*\n",
    "    1. **Context**: A patient diagnosed with Lyme disease learns about the model's high false positive rate.\n",
    "    2. **Question**: How should the patient approach the decision to accept the treatment recommended based on the model’s prediction? What steps could they take to manage their anxiety and mistrust towards the diagnosis?\n",
    "\n",
    "\n",
    "3. *Healthcare System Decision - Model Calibration*\n",
    "    1. **Context**: Administrators assess the model's impact on healthcare practices.\n",
    "    2. **Question**: Should the healthcare system recalibrate the model to adjust the **sensitivity** and **specificity** based on the current performance metrics? Consider the implications of such a recalibration on patient care, resource allocation, and ethical responsibilities of healthcare providers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14363a72",
   "metadata": {},
   "source": [
    "### 2. Ethical problems with poor practices when using proposed models [25/50 minutes]\n",
    "\n",
    "> This example will center on the `Adult Data` [dataset](https://archive.ics.uci.edu/dataset/2/adult).\n",
    "\n",
    "1. **Unfairness and Bias: Impacts of Representational and Historical Bias**\n",
    "   \n",
    "   Disproportionately imbalanced data and historical inequities within a dataset influence model predictions, potentially perpetuating past biases and affecting fairness in outcomes.\n",
    "   > We will soon see that there is a lot of imbalance across the different `Adult Data` variables.\n",
    "   > 1. Is this still proportionally representative of the current population?\n",
    "   > 2. The `Adult Data` is from the 1994 census, which was 30 years ago. Could making predictions which extrapolate past social data and trends into the present be problematic?  \n",
    "\n",
    "2. **Integrity and Credibility (\"Garbage in, garbage out\"): Model Explainability and Transparency** \n",
    "\n",
    "    It is necessary that model-based predictive outcome driven decision-making processes can be transparently understood to ensure they can be fairly and rationally evaluated by the relevant stakeholders and the trustworthiness of model can be established\n",
    "   > We will demonstrate a number of incorrect use of the `Adult Data` in a predictive model.\n",
    "   > 1. Are categorical and continuous variables treated correctly?\n",
    "   > 2. Are the relationships between the predictor variables themselves potentially problematic? \n",
    "     \n",
    "3. **Accountability of Modeling: Positive Effect or Instruments of Harm?** \n",
    "\n",
    "   We must be able to evaluate whether the application of a model serves as beneficial tools for decision-making, and challenge their use if they inadvertently become instruments of harm due to inherent underlying biases or flawed assumptions.\n",
    "\n",
    "   > The analyses below are using the `Adult Data` to predict income. \n",
    "   > 1. What are the positive applications of this model? What are the negative applications of this model? \n",
    "   > 2. If predictor variables are related, does removing a predictor variable mean it's \"effects\" are removed from a predictive model? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac89baf",
   "metadata": {},
   "source": [
    "#### A. Analysis Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "# in sklearn specificity is recall_score(y_true, y_pred, pos_label=0)\n",
    "# while sensitivity recall_score(y_true, y_pred, pos_label=1) is the default \n",
    "# precision_score = TP/(TP+TP) measuring the rate of corerct positive predictions \n",
    "# ...a useful characterization for the previous Clinical Trial considerations...\n",
    "# f1_score is a \"harmonic mean\" of precision_score and (default) recall_score\n",
    "# f1_score = 2*TP/(2*TP+FP+FN)\n",
    "\n",
    "# These will be used to examine the relationship between the predictors themselves\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3923ee9",
   "metadata": {},
   "source": [
    "#### B. Data Pre-Processing I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589a5bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "column_names = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \n",
    "    \"marital-status\", \"occupation\", \"relationship\", \"race\", \n",
    "    \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "\n",
    "data_raw = pd.read_csv(url, names=column_names, skipinitialspace=True)\n",
    "\n",
    "data_use = data_raw.copy()\n",
    "data_use = data_use.drop(columns=['workclass', 'marital-status', 'occupation', \n",
    "                                'capital-gain', 'capital-loss', 'hours-per-week', \n",
    "                                'native-country', 'education-num', 'fnlwgt'])\n",
    "\n",
    "display(data_use.head(), data_use.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b13c6c",
   "metadata": {},
   "source": [
    "#### C. Initial Exploratory Data Analysis (EDA)\n",
    "\n",
    "> Let's examine the considerations of \"1. **Unfairness and Bias: Impacts of Representational and Historical Bias**\" above. \n",
    "Interact with ChatGPT around the following prompts.\n",
    "> \n",
    "> > **Prompt 1**: \"I am going to share some data distributions from a dataset with you and would like you to summarize the potential imbalances and biases present in the data.\"\n",
    "> > - Then share, one at a time, the empirical data distributions observed below with ChatGPT and see if the responses appear correct.\n",
    "> >\n",
    "> > **Prompt 2**: \"This data is from the 1994 Census, and I'm planning to use it to create a model to predict income levels. What considerations should I take into account regarding the previous imbalances you discussed in the data when using this data to create this model?\"\n",
    "> >\n",
    "> > **Prompt 3**: What considerations should I have when using this historical data to predict present-day outcomes? What concerns might there be with these kinds of extrapolations?\n",
    ">\n",
    "> Do you think the points raised in the response from ChatGPT were correctly identified and prioritized?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28957e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imbalance data...\n",
    "education_counts = data_use['education'].value_counts()\n",
    "sex_counts = data_use['sex'].value_counts()\n",
    "race_counts = data_use['race'].value_counts()\n",
    "relationship_counts = data_use['relationship'].value_counts()\n",
    "income_counts = data_use['income'].value_counts()\n",
    "\n",
    "display(education_counts, sex_counts, race_counts, relationship_counts, income_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a7e2c5",
   "metadata": {},
   "source": [
    "#### D. Data Pre-Processing II\n",
    "\n",
    "> Let's examine the considerations of \"2. **Integrity and Credibility (\"Garbage in, garbage out\"): Model Explainability and Transparency**\" above. \n",
    "> \n",
    "> Review this \"[pre-processing](https://chatgpt.com/share/a22c47b0-d2ca-4955-b108-d24d60ce2874)\" interaction with ChatGPT and connect the noted potential concerns regarding the data preprocessing to the code below.  \n",
    "> \n",
    "> After that, then review the primary ethical concerns that were identified in the response and evaluate whether or not these represent new considerations beyond what's already been discussed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset data (for convenience)\n",
    "data_v0 = data_use.copy()\n",
    "\n",
    "# Check if any NaN values are present and warn if so\n",
    "if data_v0.isna().any().any():\n",
    "    print(\"Warning: NA values found in 'income' after mapping.\") \n",
    "\n",
    "# Standardize the age column\n",
    "scaler = StandardScaler()\n",
    "data_v0.loc[:,'age'] = scaler.fit_transform(data_use[['age']].astype(float))\n",
    "    \n",
    "# Encode categorical variables\n",
    "categorical_columns = ['income', 'education', 'relationship', 'race', 'sex']\n",
    "\n",
    "# V1\n",
    "data_v1 = data_v0.join(pd.get_dummies(data_v0[categorical_columns], \n",
    "                                      drop_first=True).astype(int))\n",
    "data_v1 = data_v1.drop(columns=categorical_columns)\n",
    "\n",
    "# V2? Why is this wrong? \n",
    "data_v2 = data_v0.join(pd.get_dummies(data_v0[categorical_columns], \n",
    "                                      drop_first=False).astype(int))\n",
    "data_v2 = data_v2.drop(columns=categorical_columns)\n",
    "data_v2 = data_v2.drop(columns='income_<=50K') # opposite outcome\n",
    "# Why are we dropping the opposite outcome this way here?\n",
    "\n",
    "# V3? Why is this wrong? \n",
    "data_v3 = data_v0.copy()\n",
    "for column in categorical_columns:\n",
    "    data_v3[column] = pd.Categorical(data_v0[column]).codes\n",
    "data_v3 = data_v3.rename(columns = {'income': 'income_>50K'}) # outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64081e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider v1, v2, and v3\n",
    "print(data_v3.dtypes)\n",
    "data_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900550f",
   "metadata": {},
   "source": [
    "> The code below uses a methodology called **Variance Inflation Factors (VIFs)** that measures **multicollinearity** which is the degree to which predictor variables are indistinguishable from the perspective of the linear model.\n",
    ">\n",
    "> - So, when `race_Black` has a large VIF then it is \"confounded\" with other variables in the model.\n",
    "> - This means the coefficient estimates in a statistical analysis are less reliable, which is reflected in an increased (less statistically significant p-value).\n",
    "> - The prescribed response to an insignificant predictor variable is to remove it from the model, but if the variable was \"confounded\" with other variables in the model then it is still indirectly a part of the model.\n",
    "> - In the `race_Black` example, this means that a model will still make predictions that are associated with `race_Black` even though this variable is not even used in the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94cdead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for problematic multicollinearity using Variance Inflation Factors (VIFs)\n",
    "\n",
    "# Consider v1, v2, and v3\n",
    "y = data_v1['income_>50K']\n",
    "X = data_v1.drop(columns='income_>50K')\n",
    "X['intercept'] = 1\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i).astype(int) \n",
    "                   for i in range(X.shape[1])]\n",
    "print(\"Variance Inflation Factor (VIF) values:\\n\", vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff3678",
   "metadata": {},
   "source": [
    "#### E. Model Fitting: Statistical Analysis\n",
    "\n",
    "> Let's examine the considerations of \"3. **Accountability of Modeling: Positive Effect or Instruments of Harm?**\" above; specifically, the following:\n",
    ">\n",
    "> > A. What are the positive applications of this model? What are the negative applications of this model? \n",
    "> >\n",
    "> > B. If predictor variables are related, does removing a predictor variable mean its \"effects\" are removed from a predictive model? \n",
    ">\n",
    "> The analysis below shows that due to **multicollinearity** (as seen through the **VIF** values) `race_Black` coefficient is not very reliable; but, for the same reason, the indirect influence of the `race_Black` variable in the model cannot be removed by simply removing the variable from the model. \n",
    ">\n",
    "> Thus, in making predictions (regardless of the inclusion or exclusion of the `race_Black` variable in the model), the predictions of this model will have observable associations with the `race_Black` variable.  Could this be misused in a harmful manner? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a6e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis\n",
    "#log_reg = sm.Logit(y_train, X_train).fit()\n",
    "log_reg = sm.Logit(y, X).fit()\n",
    "display(log_reg.summary().tables[0])\n",
    "log_reg_fit_sum = pd.read_html(log_reg.summary().tables[1].as_html(), header=0, index_col=0)[0]\n",
    "log_reg_fit_sum.loc[:,'VIF'] = vif_data.set_index('feature')\n",
    "log_reg_fit_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26220023",
   "metadata": {},
   "source": [
    "## Communication [40 minutes]\n",
    "Organize students into four groups to engage in a TA-moderated debate centered around two distinct ethical scenarios. Each scenario will be tackled by two groups, with one group defending a position and the other opposing it. Allow groups 12 minutes to discuss, prepare, and assign their argument roles, and then have the two groups debate their positions for 12 minutes in the following format\n",
    "\n",
    "> 3 different group members from the \"defending group\" will each present one argument for each of the \"Ethical Concerns\" below (with a 3 minute limit per group); then, the \"opposing group\" similarly argues three points. The remaining members from each group who have not yet argued may then provide rebuttals (with a 3 minute limit per group), starting with the \"defending team\" and then concluding with the \"opposing team\". There will then be 2 minutes for the class to reflect on the arguments and vote on the following two points: \n",
    "> 1. the effectiveness of the debating teams in presenting their points \n",
    "> 2. the position of the debating teams which students personally agree with\n",
    "\n",
    "### Scenario 1: Predictive Policing\n",
    "**Background**: A city's police department has implemented a new AI system designed to predict crime hotspots by analyzing historical crime data and demographic information. The system is intended to optimize police patrols and prevent crimes before they occur.\n",
    "\n",
    "**Debate positions**:\n",
    "- **Group A: defending the use of the AI system for policing**. Argue that predictive policing is a valuable tool for enhancing public safety and efficiently allocating police resources.\n",
    "- **Group B: opposing the use of the AI system for policing**. Contend that predictive policing perpetuates social-racial-economic bias, violates privacy, and lacks the necessary transparency and oversight.\n",
    "\n",
    "**Ethical Concerns**:\n",
    "- **Bias and Discrimination**: The AI system may inadvertently target minority communities if the historical crime data is biased.\n",
    "- **Privacy**: The use of demographic data raises concerns about the surveillance and profiling of specific groups.\n",
    "- **Transparency**: There may be limited transparency regarding how the AI system makes decisions and whether it accurately predicts crime without infringing on citizens' rights.\n",
    "\n",
    "\n",
    "### Scenario 2: Healthcare AI for Predicting Patient Risks\n",
    "**Background**: A healthcare startup has developed an AI system that uses patient data such as medical history, lifestyle choices, and genetic information to predict individuals’ risks of developing chronic diseases.\n",
    "\n",
    "**Debate Positions**:\n",
    "- **Group A: defending the use of the AI system for healthcare**. Defend the use of AI in healthcare as a means to advance preventive medicine and personalized care, potentially saving lives by early identification of risk factors.\n",
    "- **Group B: opposing the use of the AI system for healthcare**. Argue that the use of AI in healthcare could compromise patient privacy, create disparities in healthcare access, and rely on potentially flawed algorithms that could misguide medical decisions.\n",
    "\n",
    "**Ethical Concerns**:\n",
    "- **Consent and Data Usage**: Concerns about how patient data is collected, used, and shared, particularly sensitive genetic information.\n",
    "- **Accuracy and Reliability**: Potential inaccuracies in AI predictions could lead to misdiagnosis or unnecessary anxiety for patients.\n",
    "- **Access and Equality**: The AI system could lead to unequal access to healthcare if predictions are used to prioritize care or determine insurance premiums.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648cc505",
   "metadata": {},
   "source": [
    "### Optional Homework [0 minutes]\n",
    "\n",
    "> Code and write all your answers in a python notebook (in code and markdown cells) and save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking.\n",
    "\n",
    "#### F. Model Fitting: Predictive Analysis\n",
    "\n",
    "> For the `v1`, `v2`, and `v3` versions of the analysis above, copy and paste the following prompt into ChatGPT, and then provide the analysis results and ask the subsequent questions in turn and examine the responses.\n",
    ">\n",
    "> > **Prompt(s):** \"I have utilized a logistic regression model on a dataset from the 1994 Census database to predict income levels and obtained various model outputs. The steps included encoding categorical variables, standardizing age, and mapping income to binary categories. Could you help interpret the specific components of these results for some questions that I will share momentarily?\"\n",
    "> >\n",
    "> > Example submission formats:\n",
    "> > \n",
    "> > > Confusion Matrix Details:\n",
    "> > > \n",
    "> > > * True Positives: 185\n",
    "> > > * True Negatives: 5957\n",
    "> > > * False Positives: 257\n",
    "> > > * False Negatives: 1742\n",
    "> > > * Accuracy: 75%\n",
    "> > > * Precision: 42%\n",
    "> > > * Recall: 10%\n",
    "> > > * F1 Score: 16%\n",
    "> > > \n",
    "> > > Variance Inflation Factor (VIF) Multicollinearity Check:\n",
    "> > > \n",
    "> > > * Age: VIF = 1.08\n",
    "> > > * Education: VIF = 1.00\n",
    "> > > * Relationship: VIF = 1.64\n",
    "> > > * Race: VIF = 1.01\n",
    "> > > * Sex: VIF = 1.53\n",
    "> > > * Intercept: VIF = 34.47\n",
    "> > \n",
    "> > - Share the \"TN, FP, FN, TP\" results, and ask\n",
    "> >     - \"What does the proportion of false positives and false negatives suggest about the model’s specificity and sensitivity?\"\n",
    "> > - Share the \"metrics\" (Accuracy, Precision, Sensitivity, F1 Score) results\n",
    "> >     - \"How do the accuracy and precision metrics values reflect the model’s overall effectiveness in predicting correct income categories?\"\n",
    "> >     - \"Considering the sensitivity and F1 score, what are the implications for the model's ability to identify the higher income class ('>50K’)?\"\n",
    "> > - Share the \"VIF\" results from above\n",
    "> >     - \"How do the specific VIF values for each predictor variable influence our understanding of multicollinearity in this model?\"\n",
    "> > \n",
    "> > **Final Conclusion Prompt(s):**\n",
    "> > - \"Can you provide a short summary of the model’s performance, focusing on both its shortcomings and advantages based on these results? Based on the information provided, what targeted improvements could be suggested to enhance the model’s accuracy and predictive capabilities?\n",
    "> > - \"Considering the results and the important steps taken during the data preparation and analysis, what possible ethical issues should be taken into consideration for this analysis?\"\n",
    "> > - \"Does using a train-test split like `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)` alleviate the concerns about the ability of this models predictions to generalize to the present time?\n",
    "> \n",
    "> Reflecting on these interactions, write (in your own words) a 350-500 word summary discussing how the dataset might be biased relative to the current economy and how the model or statistical operations could be prone to errors. Using insights from ChatGPT, propose how you could improve the dataset handling or model to mitigate these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression(fit_intercept=True, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f6fd8",
   "metadata": {},
   "source": [
    "#### G. Model Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['Negative','Positive'])\n",
    "disp.plot()\n",
    "\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print(f'True Positive: {TP},\\nTrue Negative: {TN},\\nFalse Positive: {FP},\\nFalse Negative: {FN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96408fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)      \n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Sensitivity: {recall_score(y_test, y_pred):.2f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
