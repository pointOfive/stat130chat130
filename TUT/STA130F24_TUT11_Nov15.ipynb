{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c5662f",
   "metadata": {},
   "source": [
    "## STA130 TUT 08 (Nov15)<br><br> üå≥üå≤ <u>Decision Tree Classification with _sklearn_ and confusion matrices<u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cc949",
   "metadata": {},
   "source": [
    "### ‚ôªÔ∏è üìö Review  / Questions [30 minutes]\n",
    "\n",
    "1. **[15 of the 30 minutes]** Follow up clarification questions regarding **regression**?  Which **classification** introduced today builds directly upon?\n",
    "\n",
    "> 1. How are **predictors variables** used to **predict** an **outcome variable**?\n",
    "> 2. What's the difference between **predictive performance** and **coefficient hypothesis testing** using p-values?\n",
    "> 3. Can **R-squared** ever get worse with each additional predictor variable added to the model?\n",
    "> 4. What kind of **model performance evaluation** are we really interested in? \n",
    "> 5. Etc.?\n",
    "\n",
    "2. **[15 of the 30 minutes]** **Train-Test** \"in sample\" versus \"out of sample\" validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60dfcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c468b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = datasets.load_breast_cancer()\n",
    "cancer_df = pd.DataFrame(data=cancer_data.data, \n",
    "                         columns=cancer_data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670a35c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Notice the binary 'target' \n",
    "# with 'target_names': array(['malignant', 'benign'], dtype='<U9')\n",
    "cancer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c29e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer_data['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split into 80% training data and 20% testing data\n",
    "# Why?\n",
    "np.random.seed(131)\n",
    "training_indices = cancer_df.sample(frac=0.80, replace=False).index.sort_values()\n",
    "testing_indices = cancer_df.index[~cancer_df.index.isin(training_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab647fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the binary 'target' \n",
    "# But notice here the outcome variable is continuous (not binary) \n",
    "# (and same for the predictor variables, binary 'target' not used)\n",
    "linear_spec = 'Q(\"mean compactness\") ~ Q(\"mean radius\") + Q(\"mean concavity\")'               \n",
    "MLR = smf.ols(linear_spec, data=cancer_df.loc[training_indices,:])\n",
    "MLR_fit = MLR.fit() # Fit the mulitple lienar regression model (MLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"In sample\" performance based on the \"training data\"\n",
    "np.corrcoef(MLR_fit.predict(cancer_df.loc[training_indices,:]),\n",
    "            cancer_df.loc[training_indices,\"mean compactness\"])[0,1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Out of sample\" performance based on the \"testing data\"\n",
    "# Why?\n",
    "np.corrcoef(MLR_fit.predict(cancer_df.loc[testing_indices,:]),\n",
    "            cancer_df.loc[testing_indices,\"mean compactness\"])[0,1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69173a7",
   "metadata": {},
   "source": [
    "- The fact that the model fit as evaluated by R-squared (the \"proportion of variation explained\") is better \"data the models was trained on and has already seen\" and worse for \"new\" data is generally what we'd expect to see in an analysis like this... \n",
    "    - Why?\n",
    "\n",
    "\n",
    "- The good news is in the example above is that the \"out of sample\" predictive \"proportion of variation explained\" performance is not much worse than the \"in sample\" performance...\n",
    "- Which we'd typically interpret as suggesting that the model seems to represent the data fairly well...\n",
    "    - Why?\n",
    "\n",
    "\n",
    "- Are there any concerns about the validity of the ostinsible conclusions above? \n",
    "\n",
    "    - What might those be if so?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3008db",
   "metadata": {},
   "source": [
    "### üöß üèóÔ∏è Demo (Introducing Classification Decision Trees and Confusion Matrices) [30 minutes] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2df19c",
   "metadata": {},
   "source": [
    "#### 1. **[15 of the 30 minutes]** The concept of a *classification decision tree*\n",
    "\n",
    "Some questions to keep in mind (and hopefully answer) as you go\n",
    "\n",
    "- What's similar about this code and the **multiple linear regression code** above? \n",
    "- A **classification decision tree** is a model like **multiple linear regression**... how so?\n",
    "- **Predictions** of **classification decision tree** are made differently than those of **multiple linear regression**... how so?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "# Specify outcome and \"Design Matrix\"\n",
    "# y ~ col1 + col2 + ... + colk\n",
    "clf.fit(X=cancer_df.iloc[training_indices, :], \n",
    "        y=cancer_data.target[training_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29619d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details regarding code are not the point and will be discused and explore later in LEC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 4))\n",
    "plot_tree(clf, feature_names=cancer_data.feature_names.tolist(), \n",
    "          class_names=cancer_data.target_names.tolist(), \n",
    "          filled=True, rounded=True)\n",
    "plt.title(\"Decision Tree (Depth = 2)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452300ea",
   "metadata": {},
   "source": [
    "####  2. **[15 of the 30 minutes]** The **concept** of confusion matrix... \n",
    "\n",
    "> - Some questions to keep in mind (and hopefully answer) as you go\n",
    "> - Details regarding code are not the point and will be discused and explore later in LEC\n",
    "\n",
    "- **Classification** are **regression** are distinguised by the nature of the **outcome variables** they predict... how so? \n",
    "- What should be similar about **model performance evaluation** for **classification decision tree** to what we've seen previously?\n",
    "- Are multiple kinds of **model performance evaluation metrics** possible for **classification decision tree** based on the **confusion matrix**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Benign 0 and Malignant 1\n",
    "# JUST RUN THIS ONCE!!\n",
    "cancer_data.target = 1-cancer_data.target \n",
    "cancer_data.target_names = np.array(['Benign\\n(negative)','Malignant\\n(positive)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6893ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details regarding code are not the point and will be discused and explore later in LEC\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_depth_2 = clf.predict(cancer_df.iloc[testing_indices, :])\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(cancer_data.target[testing_indices], y_pred_depth_2)\n",
    "\n",
    "# Get the target names for 'benign' and 'malignant'\n",
    "target_names = cancer_data.target_names.tolist()\n",
    "\n",
    "# Set up a confusion matrix with proper labels using target names\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[f'Predicted {label}' for label in target_names], \n",
    "            yticklabels=[f'Actually {label}' for label in target_names])\n",
    "\n",
    "plt.title('Confusion Matrix (Depth = 2)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "# Add custom labels for FP, FN, TP, and TN\n",
    "plt.text(0.5, 0.1, \"True Negative (TN)\", fontsize=12, color='red', ha='center', va='center')\n",
    "plt.text(1.5, 0.1, \"False Positive (FP)\", fontsize=12, color='red', ha='center', va='center')\n",
    "plt.text(0.5, 1.1, \"False Negative (FN)\", fontsize=12, color='red', ha='center', va='center')\n",
    "plt.text(1.5, 1.1, \"True Positive (TP)\", fontsize=12, color='red', ha='center', va='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce69a7",
   "metadata": {},
   "source": [
    "### üí¨ üó£Ô∏è Communication Activity **[40 minutes]**\n",
    "\n",
    "1. **[7 minutes]** Break into your **course project groups** and confer regarding the purpose of the **train-test** \"in sample versus out-of-sample\" **validation** framework.\n",
    "\n",
    "2. **[6 minutes]** Identify an answer to the previous question which all groups in agreement are as close to unanimously satisfied with as possible (in the allotted time). \n",
    "   \n",
    "3. **[7 minutes]** Provide an example scenario in which the implications of **Type I erros** (wrongly rejecting the **null hypothesis**) and **Type II errors** (failing to reject a **false null hypothesis**) can be demonstrated. \n",
    "\n",
    "4. **[8 minutes]** Provide an example context involving **predictions** using a **classification decision tree** in which the implications of **false positive (FP)** and **false negative (FP) predictions** can be discussed and considered.\n",
    "\n",
    "5. **[6 minutes]** Explore with all groups what analogy they have (or could have) noticed between **Type I/Type II erros** and **false positive/negative (FP/FN) predictions**. \n",
    "\n",
    "6. **[6 minutes]** Explore with all groups what they believe the  meaningfulness, purpose, or relevance of the **train-test** \"in sample versus out-of-sample\" **validation** framework is relative to the **errors** considered here.\n",
    "\n",
    "7. If time permits, explore with all the groups whether or not they have identified any notion of different costs or consequences of the different kinds of errors that might be made.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
