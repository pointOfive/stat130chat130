{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbaf1e3",
   "metadata": {},
   "source": [
    "# STA130 TUT 07 (Nov08)<br><br> üë™ üìà <u>Multiple Linear Regression</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbec649",
   "metadata": {},
   "source": [
    "## ‚ôªÔ∏è üìö Review  / Questions [15 minutes]\n",
    "\n",
    "#### 1. Follow up questions and clarifications regarding concepts associated with the **simple linear regression** topic \n",
    "    \n",
    "> First introduced on Oct21 Oct26, and conlcluded on Nov04 and Nov07... The theoretical \"normal distribution\" regression model, model fitting, hypothesis testing, indicator variables and (two-sample) group comparision...\n",
    "> \n",
    "> - **Multiple linear regression** extends the **linear form** examined so far to multiple **predictor variables**, similarly extends the idea of **binary indicator variables** to **categorical variables**  with more than two levels, additionally provides both similar and extending **hypothesis testing** capabilities with the introduction of so-called **interaction variables** (which can be used, e.g., to examine the evidence that a linear association present between two variables differs across different groups of subpopulation groups in a dataset). \n",
    "\n",
    "<details class=\"details-example\"><summary><u><span style=\"color:blue\">The theoretical \"normal distribution\" regression model</span></u></summary>\n",
    "\n",
    "#### The theoretical \"normal distribution\" regression model\n",
    "    \n",
    "$$\\Large Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$\n",
    "\n",
    "- **Outcome** $Y_i$ is a **continuous numeric variable**\n",
    "    - **Outcome** $Y_i$ can also be called a **response**, **dependent**, or **endogenous variable** in some domains and contexts\n",
    "\n",
    "- **Predictor variable** $x_i$ is a **numeric variable**\n",
    "    - Fow now we'll consider $x_i$ to be a **continuous** numeric variable, but this is not necessary, and we will consider versions of $x_i$ later\n",
    "    - **Predictor variable** $x_i$ can also be called an **explanatory**, **independent**, or **endogenous variable**, or a **covariate** or **feature** (which are the preferred terms in the statistics and machine learning domains, respectively)\n",
    "\n",
    "- **Intercept** $\\beta_0$ and **slope** $\\beta_1$ are the two primary **parameters** of a **Simple Linear Regression** model\n",
    "    - **Intercept** and **slope** describe a **linear** (\"straight line\") relationship between **outcome** $Y_i$ and **predictor variable** $x_i$\n",
    "\n",
    "- **Error** $\\epsilon_i$ (also sometimes called the **noise**) makes **Simple Linear Regression** a **statistical model** by introducing a **random variable** with a **distribution**\n",
    "\n",
    "    - The $\\sigma^2$ **parameter** is a part of the **noise distribution** and controls how much vertical variability/spread there is in the $Y_i$ data off of the line: $\\sigma^2$ is an \"auxiliary\" **parameter** in the sense that interest is usually in $\\beta_0$ and $\\beta_1$ rather than $\\sigma^2$\n",
    "    - **Errors** $\\epsilon_i$ (in conjuction with the **linear form**) define the **assumptions** of the **Simple Linear regression** Model specification\n",
    "    - <u>but these **assumptions** are not the focus of further detailed reviewed here</u>\n",
    "    \n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>The implication of the specification on the models assumptions</u></summary>\n",
    "\n",
    "#### The implication of the specification on the models assumptions\n",
    "    \n",
    "> The first three assumptions associated with the **Simple Linear regression** model are that\n",
    "> \n",
    "> 1. the $\\epsilon_i$ **errors** (sometimes referred to as the **noise**) are **normally distributed**\n",
    "> 2. the $\\epsilon_i$ **errors** are **homoscedastic** (so their distributional variance $\\sigma^2$ does not change as a function of $x_i$)\n",
    "> 3. the linear form is [at least reasonably approximately] \"true\" (in the sense that the above two remain [at least reasonably approximately] \"true\") so that then behavior of the $Y_i$ **outcomes** are represented/determined on average by the **linear equation**)<br>\n",
    ">\n",
    ">    and there are additional assumptions; but, a deeper reflection on these is \"beyond the scope\" of STA130; nonetheless, they are that<br><br>\n",
    "> 4. the $x_i$ **predictor variable** is **measured without error**\n",
    "> 5. and the $\\epsilon_i$ **errors** are **statistically independent** (so their values do not depend on each other)\n",
    "> 6. and the $\\epsilon_i$ **errors** are **unbiased** relative to the **expected value** of **outcome** $E[Y_i|x_i]=\\beta_0 + \\beta_1x_i$ (which is equivalently stated by saying that the mean of the **error distribution** is $0$, or again equivalently, that the **expected value** of the **errors** $E[\\epsilon_i] = 0$)\n",
    "\n",
    "</details> \n",
    "  \n",
    "<details class=\"details-example\"><summary><u><span style=\"color:blue\">Model fitting for Simple Linear Regression</span></u></summary>\n",
    "\n",
    "#### Model fitting for Simple Linear Regression\n",
    "    \n",
    "> Do you remember how to use `statsmodels.ols(...).fit().summary()`? \n",
    "> \n",
    "> Hopefully so, but if not what we'll demo today with **multiple linear regression** will likely refresh your memory.\n",
    "    \n",
    "The $\\hat y_i = \\hat \\beta_0 + \\hat \\beta_1 x_i$ **fitted model** equation distinctly contrasts with the $Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$ **theoretical model** specification. To emphasize and clarify the difference, we augment our **simple linear regression** model nomenclature (as given in the \"**Review / Questions**\" section above) with the contrasting alternative notations and terminology: \n",
    "\n",
    "- **Fitted intercept** $\\hat \\beta_0$ and **slope** $\\hat \\beta_1$ ***coefficients*** are given \"hats\" to distinguish that they **estimate** (based on observed **sample data**), respectively, the **intercept** $\\beta_0$ and **slope** $\\beta_1$ ***parameters***\n",
    "\n",
    "- **Fitted (predicted) values** $\\hat y_i$ are made lower case and also given \"hats\" to distinguish them from the (upper case) **theoretical random variable** $Y_i$ implied by the **theoretical simple linear regression model** \n",
    "    - Technically, the **error** $\\epsilon_i$ is the **random variable** specified by the **simple linear regression model** specification, and this implies the **random variable** nature of $Y_i$\n",
    "\n",
    "- The **residuals** $\\text{e}_i = \\hat \\epsilon_i = y_i - \\hat y_i = y_i - \\hat \\beta_0 + \\hat \\beta_1 x_i $ also distinctly contrast with the **errors** (or **noises**) $\\epsilon_i$\n",
    "    - The **residuals** $\\text{e}_i = \\hat \\epsilon_i$ are actually available, while the **error** (or **noises**) $\\epsilon_i$ are just a theoretical concept\n",
    "    - The **residuals** $\\text{e}_i = \\hat \\epsilon_i$ nonetheless are therefore used to diagnostically assess the theoretical modeling assumptions of the  **errors** $\\epsilon_i$, such as the **normality**, **homoskedasticity**, and **linear form** assumptions; and, <u>while this is a not necessarily beyond the scope of STA130 and would certainly be a relevant consideration for the course project, this will not be addressed here at this time</u>\n",
    "\n",
    "</details> \n",
    "      \n",
    "<details class=\"details-example\"><summary><u><span style=\"color:blue\">Hypothesis testing for Simple Linear Regression</span></u></summary>\n",
    "    \n",
    "#### Hypothesis testing for Simple Linear Regression\n",
    "    \n",
    "We can use **Simple Linear Regression** to test\n",
    "    \n",
    "$$\\Large \n",
    "\\begin{align}\n",
    "H_0: {}& \\beta_1=0 \\quad \\text{ (there is no linear assocation between $Y_i$ and $x_i$ \"on average\")}\\\\\n",
    "H_A: {}& H_0 \\text{ is false}\n",
    "\\end{align}$$\n",
    "\n",
    "That is, we can assess the evidence of a linear association in the data based on a **null hypothesis** that the **slope** (the \"on average\" change in $Y_i$ per \"single unit\" change in $x_i$) is zero\n",
    "\n",
    "> We are essentially never (or only very rarely in very special circumstances) interested in a **null hypothesis** concerning the **intercept** $\\beta_0$ (as opposed to $\\beta_1$) because the assumption that $\\beta_0$ is zero essentially never (or only very rarely in very special circumstances) has any meaning, whereas the assumption that $\\beta_1$ is zero has the very practically useful interpretation of \"no linear association\" which allows us to evaluate the  evidence of a linear association based on observed data\n",
    "\n",
    "Remember, the **p-value** is \"the probability that a test statistic is as or more extreme than the observed test statistic if the null hypothesis is true\"\n",
    "\n",
    "- We do not prove $H_0$ false, we instead give evidence against the $H_0$\n",
    "     - \"We reject the null hypothesis with a p-value of abc, meaning we have xyz evidence against the null hypothesis\"\n",
    "- We do not prove $H_0$ is true, we instead do not have evidence to reject $H_0$\n",
    "     - \"We fail to reject the null hypothesis with a p-value of abc\"\n",
    "|p-value|Evidence|\n",
    "|-|-|\n",
    "|$$p > 0.1$$|No evidence against the null hypothesis|\n",
    "|$$0.1 \\ge p > 0.05$$|Weak evidence against the null hypothesis|\n",
    "|$$0.05 \\ge p > 0.01$$|Moderate evidence against the null hypothesis|\n",
    "|$$0.01 \\ge p > 0.001$$|Strong evidence against the null hypothesis|\n",
    "|$$0.001 \\ge p$$|Very strong evidence against the null hypothesis|\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary><u><span style=\"color:blue\">Indicator variables and (two-sample) group comparision</span></u></summary>\n",
    "\n",
    "#### Indicator variables and (two-sample) group comparision\n",
    "    \n",
    "A **Simple Linear Regression** can specify a model of two normally distributed populations (say, \"A\" and \"B\") with different **means** (but a common **variance**)\n",
    "    \n",
    "$$\\Large Y_i = \\beta_0 + \\beta_1 1_{[x_i=\\textrm{\"B\"}]}(x_i) + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$\n",
    "\n",
    "and where the **indicator variable** notation $1_{[x_i=\\textrm{\"B\"}]}(x_i)$ equates to $1$ whenever $x_i$ takes on the value \"B\" (as opposed to population \"A\" in this example), and $0$ otherwise. Thus, the mean of population \"A\" is $\\mu_A = \\beta_0$ while the mean of population \"B\" is $\\mu_A = \\beta_0+\\beta_1$.\n",
    "    \n",
    "A two-group comparison was also considered from a simulation perspective.\n",
    "\n",
    "- For **simulation-based hypothesis testing** we considered a **permutation test**, which approximated **the sampling distribution for an average difference statistic under the null hypothesis assumption of \"no difference between treatment groups\"** by recalculating the average difference between \"samples\" created by shuffling (or randomly permuting) the label assignments (where shuffling imposed the constraint that the created \"samples\" had the same size as the original two groups)\n",
    "\n",
    "- A **bootstrapped confidence interval** for the **difference in population means** can also be created by repeatedly resampling each of the samples (separately within each sample) to build up the **bootstrapped sampling distribution of the average difference statistics**.\n",
    "    \n",
    "Students may review the code below in their personal time at their convenience should they wish to review an explicit implementation of the above two-sample analysis specifications.\n",
    "\n",
    "```python\n",
    "# Slight editing of https://chatgpt.com/share/6b1bb97b-80b9-4a50-9323-56a51060d6c8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # Seed for reproducibility\n",
    "# Sample data... more work needed to \"parameterize\" this code to explore different sample sizes, etc.\n",
    "data = {'group': ['A'] * 10 + ['B'] * 10,  # 10 observations for each group... change loc to change group offset\n",
    "        'value': np.random.normal(loc=50, scale=5, size=10).tolist() + np.random.normal(loc=55, scale=5, size=10).tolist()}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Observed difference in means between group A and B\n",
    "observed_diff = df[df['group'] == 'A']['value'].mean() - df[df['group'] == 'B']['value'].mean()\n",
    "\n",
    "n_permutations = 1000  # Number of permutations\n",
    "permutation_diffs = np.zeros(1000)  # Store the permutation results\n",
    "\n",
    "# Permutation test\n",
    "for i in range(n_permutations):\n",
    "    # Shuffle group labels randomly; Calculate the difference in means with shuffled labels\n",
    "    shuffled = df['group'].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    shuffled_diff = df[shuffled == 'A']['value'].mean() - df[shuffled == 'B']['value'].mean()\n",
    "    permutation_diffs[i] = shuffled_diff\n",
    "\n",
    "# Calculate p-value: the proportion of permutations with an \n",
    "# absolute difference greater than or equal to the observed difference\n",
    "p_value = np.mean(np.abs(permutation_diffs) >= np.abs(observed_diff))\n",
    "print(f\"Observed difference in means: {observed_diff:.3f}\")\n",
    "print(f\"P-value from permutation test: {p_value:.3f}\")\n",
    "```\n",
    "    \n",
    "```python  \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plotting the distribution of permutation differences\n",
    "fig = px.histogram(permutation_diffs, nbins=30, title='Permutation Test: Sampling Distribution of the Difference in Means')\n",
    "fig.update_traces(marker=dict(color=\"lightblue\"), opacity=0.75)\n",
    "\n",
    "# Add a vertical line for the observed difference\n",
    "fig.add_vline(x=observed_diff, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Observed Diff\", annotation_position=\"top right\")\n",
    "\n",
    "# Highlight the area for p-value computation (two-tailed test)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=permutation_diffs[permutation_diffs >= np.abs(observed_diff)],\n",
    "    y=[0] * len(permutation_diffs[permutation_diffs >= np.abs(observed_diff)]),\n",
    "    mode='markers', marker=dict(color='red'), name=f'Extreme Values (p-value = {p_value:.3f})'))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=permutation_diffs[permutation_diffs <= -np.abs(observed_diff)],\n",
    "    y=[0] * len(permutation_diffs[permutation_diffs <= -np.abs(observed_diff)]),\n",
    "    mode='markers', marker=dict(color='red'), name='Extreme Values'))\n",
    "\n",
    "# Update layout to make the plot more informative\n",
    "fig.update_layout(xaxis_title='Difference in Means', yaxis_title='Frequency',\n",
    "                  showlegend=True, legend=dict(title=None))\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS    \n",
    "```\n",
    "\n",
    "```python\n",
    "n_bootstrap = 1000  # Number of bootstrap samples\n",
    "bootstrap_diffs = np.zeros(n_bootstrap)  # Pre-allocate a numpy array to store the bootstrap differences\n",
    "\n",
    "# Bootstrapping to create the sampling distribution of the difference in means\n",
    "for i in range(n_bootstrap):\n",
    "    # Resample with replacement for both groups A and B\n",
    "    boot_A = df[df['group'] == 'A']['value'].sample(frac=1, replace=True).reset_index(drop=True)\n",
    "    boot_B = df[df['group'] == 'B']['value'].sample(frac=1, replace=True).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate the difference in means for the bootstrap sample and store in pre-allocated array\n",
    "    bootstrap_diffs[i] = boot_A.mean() - boot_B.mean()\n",
    "\n",
    "# Calculate the 95% confidence interval (2.5th and 97.5th percentiles)\n",
    "lower_bound = np.percentile(bootstrap_diffs, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_diffs, 97.5)\n",
    "\n",
    "# Print observed difference and confidence interval\n",
    "print(f\"Observed difference in means: {observed_diff:.3f}\")\n",
    "print(f\"95% Confidence Interval: [{lower_bound:.3f}, {upper_bound:.3f}]\")\n",
    "\n",
    "# Plotting the bootstrap distribution\n",
    "fig = px.histogram(bootstrap_diffs, nbins=30, title='Bootstrapped Sampling Distribution of the Difference in Means')\n",
    "fig.update_traces(marker=dict(color=\"lightblue\"), opacity=0.75)\n",
    "\n",
    "# Add lines for the observed difference and confidence interval\n",
    "fig.add_vline(x=observed_diff, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Observed Diff\", annotation_position=\"top right\")\n",
    "fig.add_vline(x=lower_bound, line_dash=\"dot\", line_color=\"green\", annotation_text=\"Lower 95% CI\", annotation_position=\"bottom left\")\n",
    "fig.add_vline(x=upper_bound, line_dash=\"dot\", line_color=\"green\", annotation_text=\"Upper 95% CI\", annotation_position=\"bottom right\")\n",
    "\n",
    "# Update layout to make the plot more informative\n",
    "fig.update_layout(xaxis_title='Difference in Means', yaxis_title='Frequency', showlegend=False)\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS\n",
    "```    \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877d030",
   "metadata": {},
   "source": [
    "## üí¨ üó£Ô∏è Communication Activity #1 [35 minutes]\n",
    "\n",
    "Your TA will group you into <u>**SIX**</u> groups, potentially in groups that correspond to **Course Project teams**<br><br>\n",
    "\n",
    "|i | study_hours |class_section |exam_score |\n",
    "|:---|:----|:----|:----|\n",
    "|0 |10.9934280 |A |86.530831 |\n",
    "|1 |9.7234711 |A |84.632809| \n",
    "|2 |11.2953770 |B |87.036506 |\n",
    "|3 |13.0460600 |C |97.952866 |\n",
    "|4 |9.5316930 |C |79.749848|\n",
    "\n",
    "#### *[25 of the 35 minutes]* Consider data of the form given above, with two columns of **continuous** (numeric decimal) data and one column of categorical data (here with at least three different levels), and answer the following questions within your groups\n",
    "\n",
    "1. How could you use ONLY TWO **binary indicator variables** in combination to represent the ALL THREE levels (A, B, and C) in the example above?<br><br>\n",
    "\n",
    "    1. Hint 1: If $x_i$ is the `class_section` of **observation** $i$,  what do we know if $1_{[x_i=\\textrm{\"B\"}]}(x_i)$ takes on the value of $1$ (rather than $0$) \n",
    "    2. Hint 2: If $x_i$ is the `class_section` of **observation** $i$,  what do we know if $1_{[x_i=\\textrm{\"C\"}]}(x_i)$ takes on the value of $1$ (rather than $0$) \n",
    "    3. Hint 3: Using THREE **binary indicator variables** would be unnecessarily redundant for encoding the information abou which of three groups you were in... can you see why? Suppose both of the above are $0$... what does this situation indicate?<br><br> \n",
    "    \n",
    "2. What are the **means** of the different `class_section` groups in terms of the parameters of the following model specification?<br><br>\n",
    "   $$Y_i = \\beta_0 + 1_{[x_i=\\textrm{\"B\"}]}(x_i)\\beta_1 + 1_{[x_i=\\textrm{\"C\"}]}(x_i)\\beta_2 + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$<br>\n",
    "\n",
    "3. What is the nature of the data generated under the following model specification if $Y_i$ is the `exam_score` of **observation** $i$, $z_i$ is the value of `study_hours` for **observation** $i$, and $x_i$ is as described above?<br><br>\n",
    "   $$Y_i = \\beta_0 + 1_{[x_i=\\textrm{\"B\"}]}(x_i)\\beta_1 + 1_{[x_i=\\textrm{\"C\"}]}(x_i)\\beta_2 + \\beta_3 z_i + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$<br>\n",
    "\n",
    "    1. Hint: the model specification below would be described as \"a stright line relationship between the `exam_score` variable and `study_hours` variable with observed data noisily distributed around the line\". So what changes if the indicator variables are included?<br>\n",
    "    \n",
    "    $$Y_i = \\beta_0 + \\beta_3 z_i + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$<br>\n",
    "\n",
    "4. What is the practical interpretation of how `exam_score` changes relative to `class_section` according to the model specification of the previous question if $\\beta_1$ and $\\beta_2$ are not $0$?<br><br>  \n",
    "\n",
    "    1. Hint: what is the meaning if $\\beta_1$ and $\\beta_2$ are actually $0$?<br><br>  \n",
    "    \n",
    "5. What is the practical interpretation of the behavior of the relationship between `exam_score` and `study_hours` within different `class_section` groups according to the model specification of the previous question?<br><br> \n",
    "\n",
    "    1. Hint: Does it change? Does the model specification prescribe that \"the change in the outcome variable on average per unit change in the predictor\" should differ across different `class_section` groups?<br><br>  \n",
    "\n",
    "6. Is there a different kind of behavior that could be seen for the relationship between `exam_score` and `study_hours` between different `class_section` groups that might be different than what's prescribed by the model specification of the previous question?<br><br>  \n",
    "\n",
    "    1. Hint 1: what is the meaning of the following model specification?<br>\n",
    "    \n",
    "    $$Y_i = \\beta_0 + \\beta_3 z_i + 1_{[x_i=\\textrm{\"B\"}]}(x_i)\\beta_1 + \\beta_4 z_i \\times 1_{[x_i=\\textrm{\"B\"}]}(x_i) + 1_{[x_i=\\textrm{\"C\"}]}(x_i)\\beta_2 + \\beta_5 z_i \\times 1_{[x_i=\\textrm{\"C\"}]}(x_i) + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$<br>\n",
    "\n",
    "    1. Hint 2: this could also be re-expressed as...<br>\n",
    "    \n",
    "    $$Y_i = \\left(\\beta_0 +  1_{[x_i=\\textrm{\"B\"}]}(x_i)\\beta_1 + 1_{[x_i=\\textrm{\"C\"}]}(x_i)\\beta_2\\right) + \\left(\\beta_3 + \\beta_4 1_{[x_i=\\textrm{\"B\"}]}(x_i) + \\beta_5 \\right) \\times z_i + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$\n",
    "\n",
    "#### *[10 of the 35 minutes]* TAs should interrupt the group discussion every few finutes to see if all groups think they have the answer (or want a little more time), and should facilitate group progress by having one of the groups (ideally not the same group over and over) explain the answer to all the other groups (to confirm everyone has the right answer, or help move groups that are stuck along)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b1c3f",
   "metadata": {},
   "source": [
    "##  üöß üèóÔ∏è Demo (combining different kinds of predictor variables) [40 minutes]\n",
    "\n",
    "### Demo statsmodels multiple linear regression \"formula\" specifications [16 of the 40 minutes]\n",
    "\n",
    "- Get a dataset and briefly show the `statsmodels` \"formula\" implementions (https://www.statsmodels.org/dev/example_formulas.html) of the specifications below as you progress through introducing and discussing them; so, \n",
    "    - introduce, explain, discuss, and then `statsmodels` \"formula\" demo them quickly \n",
    "    - (and use a ChatBot to figure things out whenever needed)\n",
    "\n",
    "### Explain notation, terminology, and meaning [24 of the 40 minutes]\n",
    "\n",
    "1. Two (or more) **indicator predictor variables** corresponding to THREE (or more) groups based on so called \"contrasts\" (or \"offsets\") from an (arbitrarily chosen) \"baseline\" group<br><br>\n",
    "  $$\\large Y_i = \\beta_{\\textrm{A}} + 1_{[x_i=\\textrm{\"B\"}]}(x_i)\\beta_{\\textrm{B-offset}} + 1_{[x_i=\\textrm{\"C\"}]}(x_i)\\beta_{\\textrm{C-offset}} + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$<br>\n",
    "\n",
    "2. Both **indicator** and **continuous predictor variables** together in a \"parallel lines\" specification where the number of such lines increases by one for each **indicator predictor variables**<br><br>\n",
    "  $$\\large Y_i = \\beta_{\\textrm{A}} + 1_{[x_i=\\textrm{\"B\"}]}(x_i)\\beta_{\\textrm{B-offset}} + \\beta_z z_{i} + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$<br>\n",
    "\n",
    "    - Within each of the groups induced by the **indicator variables(s)**, the coefficient $\\beta_z$ is (of course) still interpreted as \"the average change in the outcome variable for a one-unit increase in the $z_{k}$ predictor variable\" but there is now also an additional difference \"on average\" (of $\\beta_{\\textrm{B-offset}}$ between groups \"A\" and \"B\" in this example) which is what produces the \"parallel lines\" nature of this specification<br><br>\n",
    "    \n",
    "3. Two (or more) **continuous predictor variables**, say $z_{1}$ and $z_{2}$ (with $z_{1i}$ and $z_{2i}$ representing observation $i$), specifying simultaneous linear relationships with the outcome variable occurring in concert<br><br>\n",
    "  $$\\large Y_i = \\beta_0 + \\beta_1 z_{1i} + \\beta_2 z_{2i} + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$<br>\n",
    "\n",
    "    - Coefficients $\\beta_k$ are still interpreted as \"the average change in the outcome variable for a one-unit increase in the $z_{k}$ predictor variable\" but now we should be careful to additionally add the stipulation \"with all other predictor variables held constant\"<br><br>\n",
    "\n",
    "4. **Interactions** (not to be confused with **indicator predictor variables**) between **continuous** and **indicator predictor variables** allowing the strength of the \"linear relationship\" between the **outcome variable** and the **continuous predictor variables** to be different within different groups, creating a \"non-parallel lines\" behavior\n",
    "\n",
    "    $$\\large Y_i = \\beta_0 + \\beta_z z_i + \\beta_{\\textrm{B-offset}} 1_{[x_i=\\textrm{\"B\"}]}(x_i) + \\beta_{\\textrm{z-change-in-B}} z_i \\times 1_{[x_i=\\textrm{\"B\"}]}(x_i) + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$<br>\n",
    "    $$\\Large\\textrm{OR}$$\n",
    "    $$\\large Y_i = \\beta_0 + \\beta_{\\textrm{B-offset}} 1_{[x_i=\\textrm{\"B\"}]}(x_i) + \\left(\\beta_z +\\beta_{\\textrm{z-change-in-B}} 1_{[x_i=\\textrm{\"B\"}]}(x_i) \\right) \\times z_i + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$<br>\n",
    "\n",
    "    - So the coefficient $\\beta_z$ is (still) interpreted as \"the average change in the outcome variable for a one-unit increase in the $z$ predictor variable within the baseline group\" but now this \"average change\" is different in other groups, so (for this example), \"the average change in the outcome variable for a one-unit increase in the $z$ predictor variable WITHIN GROUP 'B' is $\\beta_z + \\beta_{\\textrm{z-change-in-B}}$\"<br><br>\n",
    "    - And there remains here as well an additional difference \"on average\" (of $\\beta_{\\textrm{B-offset}}$ between groups \"A\" and \"B\" in this example) allowing the \"non-parallel lines\" to have different vertical shifts vertically as well as have different slopes<br><br>\n",
    "\n",
    "4. **Interactions** between **continuous predictor variables** creating a \"synergistic\" behavior between the **predictor variables** by which \"the average change in the outcome variable for a one-unit increase in one $z_k$ predictor variable depends on the value of another $z_{k'}$ predictor variable (assuming this other $z_{k'}$ predictor variable is held constant)\"<br><br>\n",
    "  $$\\large Y_i = \\beta_0 + \\beta_1 z_{1i} + \\beta_2 z_{2i} + \\beta_{12} z_{1i} \\times z_{2i} + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma^2\\right)$$<br>\n",
    " \n",
    "    - For this example, \"the average change in the outcome variable for a one-unit increase in the $z_{1}$  predictor variable (assuming $z_{2}$ predictor variable is held constant) is $\\beta_1 + \\beta_{12} z_{2}$\" which can be seen to \"synergistically\" depend on the (fixed) value of $z_{2}$\n",
    "    - And conversely, \"the average change in the outcome variable for a one-unit increase in the $z_{2}$  predictor variable (assuming $z_{1}$ predictor variable is held constant) is $\\beta_1 + \\beta_{12} z_{1}$\" which can be seen to \"synergistically\" depend on the (fixed) value of $z_{1}$<br><br>\n",
    "    - But do you see why?<br><br>\n",
    "\n",
    "6. Reading these **multiple linear regression** model specifications just requires understanding straight up simple math... the equations mean exactly what they say and should be interpreted as such; however, that said...\n",
    "\n",
    "    - One thing to be sure to keep in mind when you're interpreting the equations, though, is that you interpret them with respect to one variable at a time, under the assumption \"with all other predictor variables held constant\"\n",
    "    - This \"with all other predictor variables held constant\" assumption is \"theoretical\" in the sense that in observed data you probably can't \"make a one-unit change to just one predictor variable while simultanesouly holding all other predictor variables constant\"\n",
    "    - But, from the perspective of the mathematical equation, you can indeed \"theoretically\" consider this possibility, and this is indeed the perspective that is used to interpret the relationship of each single predictor variable with the outcome on a one-by-one, case-by-case basis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5565d",
   "metadata": {},
   "source": [
    "## üí¨ üó£Ô∏è Communication Activity #2<br>[in the (final 10 minutes of) time remaining<br>(or more or less depending on the time needed for the demo above...)]\n",
    "\n",
    "Return to you <u>**SIX**</u> groups of the first **Communication Activity**, ideally those corresponding to the **Course Project teams** of the TUT\n",
    "\n",
    "1. Discuss **Individual Project Proposals** that might relate to **multiple linear regression** and work to write out a model specification that you could analyze for the **Canadian Social Connection Survey** dataset variables for you **Course Project**\n",
    "2. Explore possible ideas for **outcome variables** (and related relavent **predictor variables**) that you might examine for your **Course Project**\n",
    "3. Consider which of the kinds of **predictor variables** could (and should) be used for your specification, and if you can imagine other **multiple linear regression** model specifications that might take advantage of the variety of types of **predictor variables** that can be leveragecd in the **multiple linear regression** context. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
