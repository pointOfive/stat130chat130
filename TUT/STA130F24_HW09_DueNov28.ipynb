{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004b5085",
   "metadata": {},
   "source": [
    "# STA130 Homework 09 [Optional]\n",
    "\n",
    "**As this is your final tutorial assignment, and with project presentations next week and finals coming up, this week’s homework will be optional. You can choose questions that interest you, complete them, and submit them if you wish. It won’t count toward your final grade, but feel free to submit it if you’d like feedback.  This assignment will focus on ethical and professional considerations we’ve covered throughout the semester.** Please see the course [wiki-textbook](https://github.com/pointOfive/STA130_ChatGPT/wiki) for the list of topics covered in this homework assignment, and a list of topics that might appear during ChatBot conversations which are \"out of scope\" for the purposes of this homework assignment (and hence can be safely ignored if encountered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131e24d",
   "metadata": {},
   "source": [
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Introduction</u></summary>\n",
    "\n",
    "A reasonable characterization of STA130 Homework is that it simply defines a weekly reading comprehension assignment. \n",
    "Indeed, STA130 Homework essentially boils down to completing various understanding confirmation exercises oriented around coding and writing tasks.\n",
    "However, rather than reading a textbook, STA130 Homework is based on ChatBots so students can interactively follow up to clarify questions or confusion that they may still have regarding learning objective assignments.\n",
    "\n",
    "> Communication is a fundamental skill underlying statistics and data science, so STA130 Homework based on ChatBots helps practice effective two-way communication as part of a \"realistic\" dialogue activity supporting underlying conceptual understanding building. \n",
    "\n",
    "It will likely become increasingly tempting to rely on ChatBots to \"do the work for you\". But when you find yourself frustrated with a ChatBots inability to give you the results you're looking for, this is a \"hint\" that you've become overreliant on the ChatBots. Your objective should not be to have ChatBots \"do the work for you\", but to use ChatBots to help you build your understanding so you can efficiently leverage ChatBots (and other resources) to help you work more efficiently.<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Instructions</u></summary>\n",
    "\n",
    "1. Code and write all your answers (for both the \"Prelecture\" and \"Postlecture\" HW) in a python notebook (in code and markdown cells) \n",
    "    \n",
    "    > It is *suggested but not mandatory* that you complete the \"Prelecture\" HW prior to the Monday LEC since (a) all HW is due at the same time; but, (b) completing some of the HW early will mean better readiness for LEC and less of a \"procrastentation cruch\" towards the end of the week...\n",
    "    \n",
    "2. Paste summaries of your ChatBot sessions (including link(s) to chat log histories if you're using ChatGPT) within your notebook\n",
    "    \n",
    "    > Create summaries of your ChatBot sessions by using concluding prompts such as \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\" or, \"Please provide me with the final working verson of the code that we created together\"\n",
    "    \n",
    "3. Save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Prompt Engineering?</u></summary>\n",
    "\n",
    "The questions (as copy-pasted prompts) are designed to initialize appropriate ChatBot conversations which can be explored in the manner of an interactive and dynamic textbook; but, it is nonetheless **strongly recommendated** that your rephrase the questions in a way that you find natural to ensure a clear understanding of the question. Given sensible prompts the represent a question well, the two primary challenges observed to arise from ChatBots are \n",
    "\n",
    "1. conversations going beyond the intended scope of the material addressed by the question; and, \n",
    "2. unrecoverable confusion as a result of sequential layers logial inquiry that cannot be resolved. \n",
    "\n",
    "In the case of the former (1), adding constraints specifying the limits of considerations of interest tends to be helpful; whereas, the latter (2) is often the result of initial prompting that leads to poor developments in navigating the material, which are likely just best resolve by a \"hard reset\" with a new initial approach to prompting.  Indeed, this is exactly the behavior [hardcoded into copilot](https://answers.microsoft.com/en-us/bing/forum/all/is-this-even-normal/0b6dcab3-7d6c-4373-8efe-d74158af3c00)...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1197d",
   "metadata": {},
   "source": [
    "### \"Prelecture\" HW [*completion prior to next LEC is suggested but not mandatory*]\n",
    "\n",
    "For the first part of this homework (Pre-lecture), you’ll need to revisit your Week 9 tutorial notebook, specifically under the optional homework sections: F. Model Fitting: Predictive Analysis and G. Model Metrics. Follow the instructions there and complete a 350-500 word interaction summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479191ac",
   "metadata": {},
   "source": [
    "> Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ce58ef",
   "metadata": {},
   "source": [
    "### \"Postlecture\" HW [*submission, along with the “Prelecture” homework, is due on November 28 if you intend to submit it]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d0779",
   "metadata": {},
   "source": [
    "1. Reflecting on the Space Shuttle Challenger disaster, where selective data analysis led to a flawed decision, consider a scenario in your field where incomplete data could similarly result in biased conclusions. For example, imagine conducting a hypothesis test to determine the effectiveness of a new medication, where some trials show no effect and are labeled as “failures.”\n",
    "    <br></br>\n",
    "    1. Describe this scenario to a ChatBot and ask why excluding these “failed” trials might lead to biased conclusions. And summarize the ChatBot’s explanation on why excluding these trials could be ethically problematic.\n",
    "    <br></br>\n",
    "\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Week1 Ethical Professionalism Considerations Recap</u></summary>\n",
    "If the observed data is \"no events occured\" does this mean the data is \"missing\" and [should be ignored](https://priceonomics.com/the-space-shuttle-challenger-explosion-and-the-o)?\n",
    "\n",
    "- NASA: \\<determines temperature doesn't affects \"o-ring\" by subseting data to just \"o-ring\" incidents\\>\n",
    "- Also NASA: \\<launches the shuttle on a cold day\\>\n",
    "\n",
    "|No apparent \"o-ring\" failure and temperature relationship|Apparent between \"o-ring\" failure and temperature relationship|\n",
    "|:-|:-|\n",
    "|if you just look at \"o-ring\" failure event data|if you instead look at ALL the data as you should|\n",
    "|![](https://etzq49yfnmd.exactdn.com/wp-content/uploads/2022/03/image06-14.png)|![](https://etzq49yfnmd.exactdn.com/wp-content/uploads/2022/03/image02-33.png)|\n",
    "|![](https://upload.wikimedia.org/wikipedia/commons/8/8b/Shuttle_Challenger_explosion.gif?20190203170223)|![](https://i.makeagif.com/media/10-04-2014/nT57xW.gif)|\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ddf481",
   "metadata": {},
   "source": [
    "2. Reproducibility is a cornerstone of ethical data analysis, especially in light of the ongoing reproducibility crisis in science.\n",
    "    <br></br>\n",
    "\t1. Use a ChatBot to explore how Jupyter notebooks and GitHub can be leveraged to make your data analysis projects more reproducible. After your exploration, summarize how you would apply these tools to ensure others can easily follow and replicate your work.\n",
    "    <br></br>\n",
    "\t2. Think about a situation where you’ve used both open-source and proprietary software in a project. How might the use of proprietary tools create obstacles for someone trying to replicate your results? Discuss strategies you could use to ensure your work remains accessible and reproducible, even when proprietary software is involved.\n",
    "    <br></br>\n",
    "\n",
    "<details class=\"details-example\">\n",
    "<summary style=\"color:blue\"><u>Week2 Ethical Professionalism Considerations Recap</u></summary>\n",
    "    \n",
    "1. **If you've not heard of the \"reproducibility crisis\" in science, have a ChatBot explain it to you**  \n",
    "   The \"reproducibility crisis\" refers to the growing concern that many scientific studies cannot be replicated or reproduced, raising doubts about the reliability of research findings.\n",
    "\n",
    "2. **If you've not heard of the \"open source software\" (versus proprietary software), have a ChatBot explain it to you**  \n",
    "   Open source software is freely available software whose source code can be modified and shared, unlike proprietary software, which is owned and its source code is usually inaccessible.\n",
    "\n",
    "3. **\"Reproducibility\" can also be considered at the level of a given data analysis project: can others replicate the results of code or analysis that you've done?**  \n",
    "    1. **Discuss with a ChatBot how jupyter notebooks and github can be used to facilitate transparency and reproducibility in data analysis**  \n",
    "       Jupyter Notebooks enable the integration of code, data, and narrative, while GitHub provides version control and sharing capabilities, together making it easier to ensure that others can replicate and verify your data analysis.\n",
    "\n",
    "4. **Discuss with a ChatBot what the distinction is between replicability of scientific experiments, versus the replicability of a specific data analysis project, and what your responsibility as an analyst should be with respect to both**  \n",
    "   Replicability of scientific experiments involves recreating the study under similar conditions to verify results, while replicability in data analysis focuses on reproducing results from the same dataset using the provided code; as an analyst, you are responsible for ensuring your analysis is transparent, documented, and reproducible.\n",
    "\n",
    "5. **Do you think proprietary (non \"open source software\") software, such as Microsoft Word, Outlook, and Copilot tends to result in high quality products?**  \n",
    "   Proprietary software often provides high-quality products due to dedicated resources and commercial incentives, but it can also limit user control and innovation.  \n",
    "    1. **Do you think software product monopolies (such as the UofT dependence on Microsoft products) makes the world a better place?**  \n",
    "       Software monopolies can lead to reduced competition, stifling innovation and potentially increasing costs, which may not be beneficial for users in the long term.\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041308e",
   "metadata": {},
   "source": [
    "3. Imagine you are a data analyst working for a national public health agency. You have been tasked with presenting data on the effectiveness of a new public health intervention designed to reduce obesity rates among adolescents. The intervention was rolled out in multiple regions, and you are compiling the results to share with policymakers who will decide on future funding. \n",
    "\n",
    "    The initial data shows a modest reduction in obesity rates, but your team is under pressure to show more significant results to secure ongoing funding. To make the intervention appear more successful, you create a bar chart comparing obesity rates before and after the intervention. However, instead of using a zero baseline, you truncate the baseline to make the reduction in obesity rates seem more dramatic than it actually is. Additionally, you selectively highlight data from regions where the intervention was most successful, while downplaying or omitting data from regions where the intervention had little to no effect.\n",
    "    <br></br>\n",
    "    1. Describe this scenario to a ChatBot and ask how truncating the baseline and selectively presenting data could mislead policymakers into overestimating the intervention’s success.\n",
    "    <br></br>\n",
    "    \n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Week3 Ethical Professionalism Considerations Recap</u></summary>\n",
    "\n",
    "|![](https://handsondataviz.org/images/14-detect/gdp-baseline-merged-annotated.png)|\n",
    "|-|\n",
    "| |\n",
    "\n",
    "Mark Twain's statment that, \"There are lies, damn lies, and statistics\", reflects a general skepticism towards statistical analysis that has been reinforced through through popular books such as [How to Lie with Statistics](https://en.wikipedia.org/wiki/How_to_Lie_with_Statistics). One place \"statistics\" can be used to decieve is through misuse of charts.  As discussed [here](https://handsondataviz.org/how-to-lie-with-charts.html) and many other places, a primary tactic that can be used to give a misleading impression using a chart is the manipulation of axes or the addition of additional dimensions which distort the meaning of size. **What are the problems with the following graphs?**\n",
    "\n",
    "|![](https://images.ctfassets.net/jicu8fwm4fvs/260tj0wxTFCAlbf4yTzSoy/2b002a49921831ab0dc05415616a1652/blog-misleading-gun-deaths-graph.jpeg)|![](https://photos1.blogger.com/blogger/5757/110/1600/macgraph.jpg)|\n",
    "|-|-|\n",
    "| | |\n",
    "\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd145c3",
   "metadata": {},
   "source": [
    "4. You are working on a project to evaluate the success of a new government initiative aimed at promoting electric vehicle (EV) adoption across the country. The survey data you’ve collected mostly comes from urban areas with ample charging infrastructure, while responses from rural areas, where charging stations are scarce, are underrepresented.\n",
    "    <br></br>\n",
    "    1. Discuss with a ChatBot how the underrepresentation of rural respondents could skew the results and lead to overly optimistic conclusions about the initiative’s success. Summarize why this could be ethically concerning, particularly when making policy decisions that affect both urban and rural populations.\n",
    "    <br></br>\n",
    "    2. Ask the ChatBot how applying bootstrapping and confidence intervals could help address this sampling bias. Summarize how these methods could lead to more accurate and fair recommendations for future EV infrastructure investments.\n",
    "    <br></br>\n",
    "    \n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Week4 Ethical Professionalism Considerations Recap</u></summary>\n",
    "\n",
    "1. **What is the difference between reporting a sample statistic (say, from the Canadian Social Connection Survey) as opposed to a population parameter (characterizing the population of the Canadians the Canadian Social Connection Survey samples)?**  \n",
    "   A sample statistic is a measure derived from a subset of the population, while a population parameter represents the entire population, and the former is used to estimate the latter.\n",
    "\n",
    "2. **Why should bootstrapping (and confidence intervals in particular) be utilized when reporting sample statistics (say, from the Canadian Social Connection Survey)?**  \n",
    "   Bootstrapping and confidence intervals provide a range of plausible values for the population parameter, giving a measure of the uncertainty associated with the sample statistic.\n",
    "\n",
    "3. **How does bootstrapping (and confidence intervals in particular) help us relate the data we have to all Canadians?**  \n",
    "   Bootstrapping generates multiple resamples from the data to estimate the variability of the sample statistic, and confidence intervals offer a range that likely contains the true population parameter, thus allowing generalization from the sample to the broader population.\n",
    "\n",
    "4. **Is the population that the Canadian Social Connection Survey samples really actually all Canadians? Or is it biased in some way?**  \n",
    "   The survey may be biased if certain groups are underrepresented or overrepresented, meaning it may not accurately reflect the diversity of the entire Canadian population.\n",
    "\n",
    "5. **Why are the previous questions \"Ethical\" and \"Professional\" in nature?**  \n",
    "   These questions are ethical and professional because they concern the accuracy, fairness, and integrity of reporting results that can influence public perception and policy decisions.\n",
    "\n",
    "6. **If the Canadian Social Connection Survey samples Canadians in some sort of biased way, how could we begin considering if the results can generalize to all Canadians; or, perhaps, the degree to which the results could generalize to all Canadians?**  \n",
    "   We could assess the sampling methods, compare demographic distributions, apply weighting techniques, and evaluate how the bias might affect the generalizability of the results.\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ce0be",
   "metadata": {},
   "source": [
    "5. You are leading a clinical trial to test whether a new diet plan lowers cholesterol levels more effectively than a standard diet. After running your analysis, you find that the p-value is 0.06, just above the traditional threshold of 0.05. Some team members suggest dismissing the diet plan as ineffective based on this result, but you’re concerned that the small sample size in your study may have impacted the outcome.\n",
    "    <br></br>\n",
    "    1. Discuss with a ChatBot why dismissing the diet plan as ineffective based solely on a p-value of 0.06 might be problematic. Summarize how this could lead to overlooking a potentially effective intervention due to the limitations of hypothesis testing and the risk of a Type II error.\n",
    "    <br></br>\n",
    "    2. Ask the ChatBot how incorporating confidence intervals alongside p-values could offer a more comprehensive assessment of the diet plan’s effectiveness. Summarize how this approach could help ensure that your conclusions are both accurate and ethically responsible.\n",
    "    <br></br>\n",
    "    \n",
    "<details class=\"details-example\">\n",
    "<summary style=\"color:blue\"><u>Week5 Ethical Professionalism Considerations Recap</u></summary>\n",
    "\n",
    "Using p-values and hypothesis testing appropriately is an important ethical and professional responsibility of anyone doing data analysis. Actually, there is quite the quiet Contra-Versy (or is it Con-TROV-ersy?) around p-values. First, on a general level, it seems quite clear that p-values and hypothesis testing methodologies MUST play some ongoing contributing role in the so-called \"replication crisis\" rampantly afflicting mordern science; namely, \"significant findings\" made in scientific studies are not able to be reproduced by future studies at an alarming rate; and, this whole paradigm of \"significant findings\" is based on p-values and hypothesis testing... so, something's going on with this methodology in some way...\n",
    "    \n",
    "More specifically however, p-values are themselves quite problematic. To see this, just briefly consider the following article titles...\n",
    "\n",
    "- [Why are p-values controversial?](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1277161) \n",
    "- [What a nerdy debate about p-values shows about science and how to fix it](https://www.vox.com/science-and-health/2017/7/31/16021654/p-values-statistical-significance-redefine-0005)\n",
    "- [The reign of the p-value is over: what alternative analyses could we employ to fill the power vacuum?](https://royalsocietypublishing.org/doi/10.1098/rsbl.2019.0174)\n",
    "- [Scientists rise up against statistical significance](https://www.nature.com/articles/d41586-019-00857-9)\n",
    "- [Statistics experts urge scientists to rethink the p-value](https://www.spectrumnews.org/news/statistics-experts-urge-scientists-rethink-p-value)\n",
    "\n",
    "While the issues here are relatively advanced and subtle (as introduced [here](https://www2.stat.duke.edu/~berger/p-values.html), presented [here](https://www.jarad.me/courses/stat587Eng/slides/Inference/I06-Pvalues/why_pvalues_dont_mean_what_you_think_they_mean.pdf), and demonstrated using simulation [here](https://jaradniemi.shinyapps.io/pvalue/)), the problem essentially comes down to the fact that most scientists (or just people) don't know how to really interpret the numeric value of a p-value. There are therefore two current proposed solutions to address this challenge.\n",
    "    \n",
    "1. Just interpreting p-values using the follwing table (which really isn't that hard, so it's surprising that this solution isn't more broadly adopted...)\n",
    "    \n",
    "|p-value|Evidence|\n",
    "|-|-|\n",
    "|$$p > 0.1$$|No evidence against the null hypothesis|\n",
    "|$$0.1 \\ge p > 0.05$$|Weak evidence against the null hypothesis|\n",
    "|$$0.05 \\ge p > 0.01$$|Moderate evidence against the null hypothesis|\n",
    "|$$0.01 \\ge p > 0.001$$|Strong evidence against the null hypothesis|\n",
    "|$$0.001 \\ge p$$|Very strong evidence against the null hypothesis|\n",
    "    \n",
    "\n",
    "2. Only do **hypothesis testing** on the basis of confidence intervals, not **p-values** (which might be the best solution wherever doing so is a realistic, convenient  possibility...)\n",
    "\n",
    "With this quite broad introductory context in mind, what does your favorite ChatBot thinks about the following statements? \n",
    "    \n",
    "1. Hypothesis testing is not a \"mathematical proof\"<br><br>\n",
    "\n",
    "    1. We do not prove $H_0$ false, we instead give evidence against the $H_0$: \"We reject the null hypothesis with a p-value of XYZ, meaning we have ABC evidence against the null hypothesis\"\n",
    "    2. We do not prove $H_0$ is true, we instead do not have evidence to reject $H_0$: \"We fail to reject the null hypothesis with a p-value of XYZ\"<br><br>\n",
    "\n",
    "2. Implying that a \"non-significant result\" means there is \"no effect\" misleads an audience because this may in actual fact simply indicate that there was insufficient evidence to reject the null hypothesis. So this therefore overlooks the possibility of sample size limitations, or Type II errors (which means a test incorrectly concludes that there is no effect or difference when, in fact, there is one). \n",
    "    \n",
    "    > Similarly, analagously, a \"significant result\" used to reject the null hypothsis could alternatively be a Type I error (which means a test actually incorrectly rejected a null hypothesis when it was actually true)... we're only providing a measure of evidence against the null hypothesis... but the evidence could still incorrectly suggest the wrong conclusion... it really depends on how strong the evidence is...\n",
    "    >\n",
    "    > - all of which is why just interpreting p-values using the table above is a good idea...\n",
    "\n",
    "3. The p-values used for hypothesis testing are contructed upone the assumptions of the null hypotheses they correspond to; but, null hypotheses are actually often presented in simple forms that routinely hide a lot of information that is implicitly used to construct the p-values. For example, distributional assumptions about the population, estimated \"plug-in\" values that can used to simplify the problem calculations, and the reliance upon \"random sampling\", etc...\n",
    "        \n",
    "    \n",
    "4. Drawing overly broad conclusions, or making recommendations based on findings that reject the null hypothesis in a specific context is fraught with the problematic risks of overgeneralization errors. Further exacerbating this issue, null hypotheses are typically so called \"point null hypotheses\" which is meant to emphasize that they are mathematically increadibly sharply specific; whereas, alternative hypotheses are usually very unspecific. An alternative hypothesis that \"the null hypothesis is false\" doesn't say much... we should wonder, \"how, specfically, is the null false?\"\n",
    "    \n",
    "   As an example really giving a demonstrating this, consider rejecting a null hypothesis that there is no correlation between rain and pizza's delivered. Such a decision doesn't specify what the actual hypothetical correlation might be. In fact, it doesn't even indicate if there are more or less pizzas delivered when it rains... \n",
    "\n",
    "    > which, actually, shows very clearly why statistical inference using hypothesis testing is inferior to statistical inference based on confidence intervals...\n",
    "    > \n",
    "    > - a confidence interval provides a range of plausible values of what the parameter in question might be; whereas, ...\n",
    "    > - trying to more clearly address what the plausible values of the parameter in question might be on the basis of hypothesis testing would require conducting further experiements to continously reject increasingly detailed hypothesies to narrow down what the alternative hypothesis might actually include... which would indeed be an utterly vapid misuse of the intended purpose of hypothesis testing entrprise... \n",
    "    \n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd090a1",
   "metadata": {},
   "source": [
    "6. You’re tasked with developing a Simple Linear Regression model to predict the success of a new marketing campaign based on the amount of money spent on advertising. Your model indicates a significant positive relationship, with a small p-value suggesting that increased spending strongly correlates with campaign success. However, after digging deeper, you notice that the data points from a few high-spending campaigns are skewing the results, and the model’s assumptions, like normally distributed errors and constant variance, might not hold.\n",
    "    <br></br>\n",
    "    1. Describe to a ChatBot how ignoring these skewed data points could lead to an overestimation of the campaign’s effectiveness. Ask the ChatBot why it’s crucial to check for assumptions such as homoscedasticity and normality in this context. Summarize how failing to do so could mislead stakeholders into making costly decisions based on unreliable results.\n",
    "    <br></br>\n",
    "    2. Ask the ChatBot for techniques to diagnose whether your model’s assumptions hold, particularly in cases where a few data points have a disproportionate influence. Summarize a strategy that ensures your conclusions are not just statistically significant but also trustworthy and reflective of the true relationship.\n",
    "    <br></br>\n",
    "    \n",
    "<details class=\"details-example\">\n",
    "<summary style=\"color:blue\"><u>Week6 Ethical Professionalism Considerations Recap</u></summary>\n",
    "The TUT and HW both addressed some of the assumptions used in **Simple Linear Regression**. The **p-values** provided by `statsmodels` via `smf.ols(...).fit()` depend on these assumptions, so if they are not (at least approximately) correct, the **p-values** (and any subsequent claims regarding the \"evidience against\" the **null hypothesis**) are not reliable. In light of this consideration, describe how you could diagnostically check the first three assumptions (given below) when using analyses based on **Simple Linear regression** model. From an Ethical and Professional perspective, do you think doing diagnostic checks on the assumptions of a **Simple Linear regression** model is something you can and should do whenever you're doing this kind of analysis? \n",
    "            \n",
    "> The first three assumptions associated with the **Simple Linear regression** model are that<br>\n",
    "> \n",
    "> 1. the $\\epsilon_i$ **errors** (sometimes referred to as the **noise**) are **normally distributed**\n",
    "> 2. the $\\epsilon_i$ **errors** are **homoscedastic** (so their distributional variance $\\sigma^2$ does not change as a function of $x_i$)\n",
    "> 3. the linear form is [at least reasonably approximately] \"true\" (in the sense that the above two remain [at least reasonably approximately] \"true\") so that then behavior of the $Y_i$ **outcomes** are represented/determined on average by the **linear equation**)<br>\n",
    "> \n",
    ">    and there are additional assumptions; but, a deeper reflection on these is \"beyond the scope\" of STA130; nonetheless, they are that<br><br>\n",
    "> 4. the $x_i$ **predictor variable** is **measured without error**\n",
    "> 5. and the $\\epsilon_i$ **errors** are **statistically independent** (so their values do not depend on each other)\n",
    "> 6. and the $\\epsilon_i$ **errors** are **unbiased** relative to the **expected value** of **outcome** $E[Y_i|x_i]=\\beta_0 + \\beta_1x_i$ (which is equivalently stated by saying that the mean of the **error distribution** is $0$, or again equivalently, that the **expected value** of the **errors** $E[\\epsilon_i] = 0$)\n",
    "    \n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a1c83",
   "metadata": {},
   "source": [
    "7. You are analyzing data to understand the factors influencing employee productivity in a large corporation. Your Multiple Linear Regression (MLR) model includes variables like hours worked, experience, and workplace environment, with an interaction term between experience and workplace environment. The model shows a significant interaction effect, suggesting that the relationship between experience and productivity changes depending on the workplace environment. However, before finalizing your report, you realize that you haven’t checked whether the assumptions of MLR, such as linearity and homoscedasticity, are met.\n",
    "    <br></br>\n",
    "    1. Discuss with a ChatBot why ignoring the assumptions of MLR could lead to incorrect conclusions about the factors influencing employee productivity. How might the significant interaction term mislead you if the model assumptions are violated? Summarize the potential risks of basing your recommendations on an improperly validated model.\n",
    "    <br></br>\n",
    "    2. Ask the ChatBot for methods to check the key assumptions of MLR, particularly in models that include interaction terms. Summarize a strategy you would use to ensure that your analysis is both statistically sound and ethically responsible, focusing on how to properly interpret interaction terms.\n",
    "    <br></br>\n",
    "\n",
    "<details class=\"details-example\">\n",
    "<summary style=\"color:blue\"><u>Week7 Ethical Professionalism Considerations Recap</u></summary>\n",
    "\n",
    "This week's tutorial emphasized **Multiple Linear Regression (MLR)**, with a particular focus on **interaction terms** and their impact on the interpretation of regression models. The validity of your MLR results, including **p-values** and coefficients, relies on the correct application of the model and the satisfaction of certain assumptions. It’s important to reflect on the ethical and professional responsibilities you have when conducting such analyses.\n",
    "\n",
    "Consider the following questions:\n",
    "\n",
    "1. **Diagnostic Checks for MLR Assumptions**:\n",
    "   - How would you go about verifying that the assumptions of **Multiple Linear Regression** are met in your analyses? Focus on:\n",
    "     - **Linearity**: Ensure that the relationships between predictors and the outcome are linear.\n",
    "     - **Independence**: Check that the residuals (errors) are independent of each other.\n",
    "     - **Homoscedasticity**: Verify that the residuals have a constant variance across all levels of the predictors.\n",
    "     - **Normality**: Confirm that the residuals are normally distributed.\n",
    "\n",
    "2. **Ethical Implications**:\n",
    "   - Reflect on the ethical importance of conducting these diagnostic checks. What could be the consequences of making decisions based on a model that violates key assumptions? How might the inclusion of **interaction terms** complicate the interpretation of your results, and what steps can you take to ensure that your conclusions are both valid and reliable?\n",
    "\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31142a5a",
   "metadata": {},
   "source": [
    "8. Imagine you’re developing a decision tree model to predict which employees at a fictional tech startup, “O.OFIVE,” are likely to leave the company based on factors like work hours, project assignments, and snack consumption (yes, snack consumption—because who doesn’t love a good snack while coding?). Your model is performing well, but you suddenly remember that your data includes personal information about employees’ snack preferences, which you collected without explicit consent. You also notice that the model sometimes wrongly predicts that employees who eat too many donuts are planning to quit, leading to awkward conversations with HR.\n",
    "    <br></br>\n",
    "    1. Discuss with a ChatBot why using personal data like snack preferences without consent could be ethically problematic, even if it leads to accurate predictions. How might this oversight impact employee trust and the company’s reputation? Summarize the potential consequences of deploying this model without addressing these ethical concerns.\n",
    "    <br></br>\n",
    "    2. Ask the ChatBot how you could redesign your data collection process to ensure informed consent and enhance transparency in your decision tree model. Summarize a fun and creative approach that could make the process more engaging for employees, ensuring both ethical data use and a positive workplace environment (maybe even with some donut incentives!).\n",
    "    <br></br>\n",
    "    \n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Week8 Ethical Professionalism Considerations Recap</u></summary>\n",
    "\n",
    "1. **Discuss with a ChatBox about consent and data collection for training models.**  \n",
    "    1. **Discuss the ethics of data collection for training decision trees, particularly the need for informed consent when personal data is involved.**  \n",
    "       Informed consent is needed when collecting personal data for training decision trees, as it ensures individuals are aware of how their data will be used, respecting their autonomy and privacy.\n",
    "    2. **Evaluate the role of regulatory frameworks in ensuring ethical data collection practices.**  \n",
    "       Regulatory frameworks play a important role in enforcing ethical data collection by setting guidelines that protect individuals' rights and ensure data is collected transparently and responsibly.\n",
    "\n",
    "2. **Discuss with a ChatBox about accountability in automated decision-making.**  \n",
    "    1. **Address the challenges of holding systems and their developers accountable when decision trees lead to adverse outcomes.**  \n",
    "       Accountability in automated decision-making is challenging due to the complexity of systems and the potential diffusion of responsibility among developers, necessitating clear guidelines for liability.\n",
    "    2. **Explore legal and ethical frameworks for responsibility when automated decisions go wrong.**  \n",
    "       Legal and ethical frameworks must define clear responsibilities for developers and users of automated systems, ensuring that there are mechanisms in place to address and rectify adverse outcomes.\n",
    "\n",
    "3. **Discuss with a ChatBox about transparency and explainability in classification models.**  \n",
    "    1. **Discuss the importance of model transparency, particularly when using decision trees in sectors like healthcare or criminal justice.**  \n",
    "       Transparency in decision tree models is crucial in sectors like healthcare or criminal justice to ensure that decisions can be understood and trusted by all stakeholders, reducing the risk of bias or unfair outcomes.\n",
    "    2. **Explore methods to enhance the explainability of decision trees, such as visualization techniques and simplified decision paths.**  \n",
    "       Enhancing explainability can be achieved through visualization techniques and simplified decision paths, which help make complex models more interpretable and accessible to non-experts.\n",
    "\n",
    "4. **Discuss with a ChatBox about the impact of misclassifications in critical applications.**  \n",
    "    1. **Examine the consequences of false positives and false negatives in decision tree outcomes, using confusion matrices to highlight these issues.**  \n",
    "       Misclassifications such as false positives and false negatives in decision tree outcomes can have significant consequences, especially in critical fields like medicine or law enforcement, where errors can lead to serious harm.\n",
    "    2. **Discuss ethical responsibilities when deploying classifiers in high-stakes fields like medicine or law enforcement.**  \n",
    "       Ethical responsibilities in deploying classifiers in high-stakes fields include rigorous testing, continuous monitoring, and being prepared to intervene when the model's decisions could cause harm.\n",
    "</details>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
