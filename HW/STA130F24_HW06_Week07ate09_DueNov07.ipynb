{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd8c93b",
   "metadata": {},
   "source": [
    "# STA130 Homework 06\n",
    "\n",
    "Please see the course [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) for the list of topics covered in this homework assignment, and a list of topics that might appear during ChatBot conversations which are \"out of scope\" for the purposes of this homework assignment (and hence can be safely ignored if encountered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991dc719",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Introduction</u></summary>\n",
    "\n",
    "### Introduction\n",
    "    \n",
    "A reasonable characterization of STA130 Homework is that it simply defines a weekly reading comprehension assignment. \n",
    "Indeed, STA130 Homework essentially boils down to completing various understanding confirmation exercises oriented around coding and writing tasks.\n",
    "However, rather than reading a textbook, STA130 Homework is based on ChatBots so students can interactively follow up to clarify questions or confusion that they may still have regarding learning objective assignments.\n",
    "\n",
    "> Communication is a fundamental skill underlying statistics and data science, so STA130 Homework based on ChatBots helps practice effective two-way communication as part of a \"realistic\" dialogue activity supporting underlying conceptual understanding building. \n",
    "\n",
    "It will likely become increasingly tempting to rely on ChatBots to \"do the work for you\". But when you find yourself frustrated with a ChatBots inability to give you the results you're looking for, this is a \"hint\" that you've become overreliant on the ChatBots. Your objective should not be to have ChatBots \"do the work for you\", but to use ChatBots to help you build your understanding so you can efficiently leverage ChatBots (and other resources) to help you work more efficiently.<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Instructions</u></summary>\n",
    "\n",
    "### Instructions\n",
    "    \n",
    "1. Code and write all your answers (for both the \"Pre-lecture\" and \"Post-lecture\" HW) in a python notebook (in code and markdown cells) \n",
    "    \n",
    "> It is *suggested but not mandatory* that you complete the \"Pre-lecture\" HW prior to the Monday LEC since (a) all HW is due at the same time; but, (b) completing some of the HW early will mean better readiness for LEC and less of a \"procrastentation cruch\" towards the end of the week...\n",
    "    \n",
    "2. Paste summaries of your ChatBot sessions (including link(s) to chat log histories if you're using ChatGPT) within your notebook\n",
    "    \n",
    "> Create summaries of your ChatBot sessions by using concluding prompts such as \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\" or, \"Please provide me with the final working verson of the code that we created together\"\n",
    "    \n",
    "3. Save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Prompt Engineering?</u></summary>\n",
    "    \n",
    "### Prompt Engineering? \n",
    "    \n",
    "The questions (as copy-pasted prompts) are designed to initialize appropriate ChatBot conversations which can be explored in the manner of an interactive and dynamic textbook; but, it is nonetheless **strongly recommendated** that your rephrase the questions in a way that you find natural to ensure a clear understanding of the question. Given sensible prompts the represent a question well, the two primary challenges observed to arise from ChatBots are \n",
    "\n",
    "1. conversations going beyond the intended scope of the material addressed by the question; and, \n",
    "2. unrecoverable confusion as a result of sequential layers logial inquiry that cannot be resolved. \n",
    "\n",
    "In the case of the former (1), adding constraints specifying the limits of considerations of interest tends to be helpful; whereas, the latter (2) is often the result of initial prompting that leads to poor developments in navigating the material, which are likely just best resolve by a \"hard reset\" with a new initial approach to prompting.  Indeed, this is exactly the behavior [hardcoded into copilot](https://answers.microsoft.com/en-us/bing/forum/all/is-this-even-normal/0b6dcab3-7d6c-4373-8efe-d74158af3c00)...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ed17b",
   "metadata": {},
   "source": [
    "### Marking Rubric (which may award partial credit) \n",
    "\n",
    "- [0.1 points]: All relevant ChatBot summaries [including link(s) to chat log histories if you're using ChatGPT] are reported within the notebook\n",
    "- [0.2 points]: Evaluation of correctness and clarity in written communication for Question \"3\"\n",
    "- [0.2 points]: Evaluation of correctness and clarity in written communication for Question \"4\"\n",
    "- [0.2 points]: Evaluation of submitted work and conclusions for Question \"9\"\n",
    "- [0.3 points]: Evaluation of written communication of the \"big picture\" differences and correct evidence assessement for Question \"11\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326d8bb",
   "metadata": {},
   "source": [
    "## \"Pre-lecture\" versus \"Post-lecture\" HW? \n",
    "\n",
    "- _**Your HW submission is due prior to the Nov08 TUT on Friday after you return from Reading Week; however,**_\n",
    "- _**this homework assignment is longer since it covers material from both the Oct21 and Nov04 LEC (rather than a single LEC); so,**_\n",
    "- _**we'll brake the assignment into \"Week of Oct21\" and \"Week of Nov04\" HW and/but ALL of it will be DUE prior to the Nov08 TUT**_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc8cd7",
   "metadata": {},
   "source": [
    "## \"Week of Oct21\" HW [*due prior to the Nov08 TUT*]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8140d3a",
   "metadata": {},
   "source": [
    "### 1. Explain the theoretical Simple Linear Regression model in your own words by describing its components (of predictor and outcome variables, slope and intercept coefficients, and an error term) and how they combine to form a sample from normal distribution; then, create *python* code explicitly demonstrating your explanation using *numpy* and *scipy.stats* <br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _Your answer can be quite concise and will likely just address the \"mathematical\" and \"statistical\" aspects of the process of a **Simple Linear Model** specification, perhaps giving an intuitive interpretation summary of the result as a whole_\n",
    ">   \n",
    "> - _Your code could be based on values for `n`, `x`, `beta0`, `beta1`, and `sigma`; and, then create the `errors` and `Y`_\n",
    "> \n",
    "> - _The predictors $x_i$ can be fixed arbitrarily to start the process (perhaps sampled using `stats.uniform`), and they are conceptually different from the creation of **error** (or **noise**) terms $\\epsilon_i$ which are sampled from a **normal distribution** (with some aribtrarily *a priori* chosen **standard deviation** `scale` parameter $\\sigma$) which are then combined with $x_i$ through the **Simple Linear Model** equation (based on aribtrarily *a priori* chosen **slope** and **intercept coefficients**) to produce the $Y_i$ outcomes_\n",
    "> \n",
    "> - _It should be fairly easy to visualize the \"a + bx\" line defined by the **Simple Linear Model** equation, and some **simulated** data points around the line in a `plotly` figure using the help of a ChatBot_\n",
    "> \n",
    "> _If you use a ChatBot (as expected for this problem), **don't forget to ask for summaries of your ChatBot session(s) and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatGPT)**_\n",
    ">\n",
    "> \n",
    "> _**Question Scope Warning:** Be careful when using a ChatBot to help you with creating an example dataset and coding up a visualization though, **because it might suggest creating (and visualizing) a fitted model for to your data (rather than the theoretical model); but, this is not what this question is asking you to demonstrate**. This question is not asking about how to produce a fitted **Simple Linear Regression** model or explain how model **slope** and **intercept coefficients** are calculated (e.g., using \"ordinary least squares\" or analytical equations to estimate the **coefficients**  for an observed dataset)._\n",
    "> \n",
    "> ```python\n",
    "> # There are two distinct ways to use `plotly` here\n",
    ">\n",
    "> import plotly.express as px\n",
    "> px.scatter(df, x='x',  y='Y', color='Data', \n",
    ">            trendline='ols', title='Y vs. x')\n",
    ">        \n",
    "> import plotly.graph_objects as go\n",
    "> fig = go.Figure()\n",
    "> fig.add_trace(go.Scatter(x=x, y=Y, mode='markers', name='Data'))\n",
    "> \n",
    "> # The latter is preferable since `trendline='ols'` in the former \n",
    "> # creates a fitted model for the data and adds it to the figure\n",
    "> # and, again, THAT IS NOT what this problem is asking for right now\n",
    "> ```\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot) But if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f9cdf",
   "metadata": {},
   "source": [
    "### 2. Use a dataset simulated from your theoretical Simple Linear Regression model to demonstrate how to create and visualize a fitted Simple Linear Regression model using *pandas* and *import statsmodels.formula.api as smf*<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> - _Combine the **simulated** `x` and `Y` into a `pandas` data frame object named `df` with the column names \"x\" and \"Y\"_\n",
    "> \n",
    "> - _Replace the inline question comments below with their answers (working with a ChatBot if needed)_\n",
    ">\n",
    "> ```python\n",
    "> import statsmodels.formula.api as smf  # what is this library for?\n",
    "> import plotly.express as px  # this is a ploting library\n",
    ">\n",
    "> # what are the following two steps doing?\n",
    "> model_data_specification = smf.ols(\"Y~x\", data=df) \n",
    "> fitted_model = model_data_specification.fit() \n",
    ">\n",
    "> # what do each of the following provide?\n",
    "> fitted_model.summary()  # simple explanation? \n",
    "> fitted_model.summary().tables[1]  # simple explanation?\n",
    "> fitted_model.params  # simple explanation?\n",
    "> fitted_model.params.values  # simple explanation?\n",
    "> fitted_model.rsquared  # simple explanation?\n",
    ">\n",
    "> # what two things does this add onto the figure?\n",
    "> df['Data'] = 'Data' # hack to add data to legend \n",
    "> fig = px.scatter(df, x='x',  y='Y', color='Data', \n",
    ">                  trendline='ols', title='Y vs. x')\n",
    ">\n",
    "> # This is essentially what above `trendline='ols'` does\n",
    "> fig.add_scatter(x=df['x'], y=fitted_model.fittedvalues,\n",
    ">                 line=dict(color='blue'), name=\"trendline='ols'\")\n",
    "> \n",
    "> fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS\n",
    "> ```\n",
    ">\n",
    "> _The plotting here uses the `plotly.express` form `fig.add_scatter(x=x, y=Y)` rather than the `plotly.graph_objects` form `fig.add_trace(go.Scatter(x=x, y=Y))`. The difference between these two was noted in the \"Further Guidance\" comments in the previous question; but, the preference for the former in this case is because `px` allows us to access `trendline='ols'` through `px.scatter(df, x='x',  y='Y', trendline='ols')`_\n",
    ">\n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot) But if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_      \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b70bc0",
   "metadata": {},
   "source": [
    "### 3. Add the line from Question 1 on the figure of Question 2 and explain the difference between the nature of the two lines in your own words; *but, hint though: simulation of random sampling variation*<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _This question is effectively asking you to explain what the combined code you produced for Questions 1 and 2 is trying to demonstrate overall. If you're working with a ChatBot (as expected), giving these two sets of code as context, and asking what the purpose of comparing these lines could be would be a way to get some help in formulating your answer_\n",
    "> \n",
    "> _The graphical visualization aspect of this question could be accomplished by appending the following code to the code provided in Question 2._\n",
    "> \n",
    "> ```python\n",
    "> # what does this add onto the figure in constrast to `trendline='ols'`?\n",
    "> x_range = np.array([df['x'].min(), df['x'].max()])\n",
    "> # beta0 and beta1 are assumed to be defined\n",
    "> y_line = beta0 + beta1 * x_range\n",
    "> fig.add_scatter(x=x_range, y=y_line, mode='lines',\n",
    ">                 name=str(beta0)+' + '+str(beta1)+' * x', \n",
    ">                 line=dict(dash='dot', color='orange'))\n",
    ">\n",
    "> fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS\n",
    "> ```\n",
    "> \n",
    "> _The right way to interactively \"see\" the answer to this question is to repeatedly create different dataset **simulations** using your theoretical model and the corresponding fitted models, and repeatedly visualize the data and the two lines over and over... this would be as easy as rerunning a single cell containing your simulation and visualization code..._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot) But if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68e71c",
   "metadata": {},
   "source": [
    "### 4. Explain how *fitted_model.fittedvalues* are derived on the basis of *fitted_model.summary().tables[1]* (or more specifically  *fitted_model.params* or *fitted_model.params.values*)<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _The previous questions used code to explore the distinction between theoretical (true) $Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\;[\\text{where } \\epsilon_i \\sim \\mathcal{N}(0, \\sigma)]\\;$ and fitted (estimated) $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ **Simple Linear Regression** models_\n",
    ">\n",
    "> _This question asks you to explicitly illustrate how the the latter \"in sample predictions\" of the fitted **Simple Linear Regression** model $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ are made (in contrast to the linear equation of the theoretical model)_\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot) But if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9fdfa9",
   "metadata": {},
   "source": [
    "### 5. Explain concisely in your own words what line is chosen for the fitted model based on observed data using the \"ordinary least squares\" method (as is done by *trendline='ols'* and *smf.ols(...).fit()*) and why it requires \"squares\"<br>\n",
    "    \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _This question addresses the use of **residuals** $\\text{e}_i = \\hat \\epsilon_i = Y_i - \\hat y_i$ (in contrast to the **error** terms $\\epsilon_i$ of the theoretical model), and particularly, asks for an explanation based on the following visualization_\n",
    ">\n",
    "> ```python \n",
    "> import scipy.stats as stats\n",
    "> import numpy as np\n",
    "> import pandas as pd\n",
    "> import statsmodels.formula.api as smf\n",
    "> import plotly.express as px\n",
    "> \n",
    "> n,x_min,x_range,beta0,beta1,sigma = 20,5,5,2,3,5\n",
    "> x = stats.uniform(x_min, x_range).rvs(size=n)\n",
    "> errors = stats.norm(loc=0, scale=sigma).rvs(size=n)\n",
    "> Y = beta0 + beta1 * x + errors\n",
    "> \n",
    "> df = pd.DataFrame({'x': x, 'y': Y})\n",
    "> model_data_specification = smf.ols(\"Y~x\", data=df) \n",
    "> fitted_model = model_data_specification.fit() \n",
    "> \n",
    "> df['Data'] = 'Data' # hack to add data to legend \n",
    "> fig = px.scatter(df, x='x',  y='Y', color='Data', \n",
    ">                  trendline='ols', title='Y vs. x')\n",
    "> \n",
    "> # This is what `trendline='ols'` is\n",
    "> fig.add_scatter(x=df['x'], y=fitted_model.fittedvalues,\n",
    ">                 line=dict(color='blue'), name=\"trendline='ols'\")\n",
    "> \n",
    "> x_range = np.array([df['x'].min(), df['x'].max()])\n",
    "> y_line = beta0 + beta1 * x_range\n",
    "> fig.add_scatter(x=x_range, y=y_line, mode='lines',\n",
    ">                 name=str(beta0)+' + '+str(beta1)+' * x', \n",
    ">                 line=dict(dash='dot', color='orange'))\n",
    "> \n",
    "> # Add vertical lines for residuals\n",
    "> for i in range(len(df)):\n",
    ">     fig.add_scatter(x=[df['x'][i], df['x'][i]],\n",
    ">                     y=[fitted_model.fittedvalues[i], df['Y'][i]],\n",
    ">                     mode='lines',\n",
    ">                     line=dict(color='red', dash='dash'),\n",
    ">                     showlegend=False)\n",
    ">     \n",
    "> # Add horizontal line at y-bar\n",
    "> fig.add_scatter(x=x_range, y=[df['Y'].mean()]*2, mode='lines',\n",
    ">                 line=dict(color='black', dash='dot'), name='y-bar')\n",
    "> \n",
    "> fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS\n",
    "> ```\n",
    ">\n",
    "> _**Question Scope Warning**: we are not looking for any explanation realted to the mathematical equations for the line chosen for the **Simple Linear Regression** model by the \"ordinary least squares\" method, which happen to be_\n",
    "> \n",
    "> $$\\hat \\beta_1 = r_{xy}\\frac{s_y}{s_x} \\quad \\text{ and } \\quad  \\hat\\beta_0 = \\bar {y}-\\hat \\beta_1\\bar {x}$$\n",
    ">\n",
    "> where $r_{xy}$ is the **correlation** between $x$ and $Y$ and $s_x$ and $s_Y$ are the **sample standard deviations** of $x$ and $y$\n",
    ">\n",
    "> ---\n",
    "> \n",
    "> ```python \n",
    "> # Use this if you need it    \n",
    "> # https://stackoverflow.com/questions/52771328/plotly-chart-not-showing-in-jupyter-notebook\n",
    "> import plotly.offline as pyo\n",
    "> # Set notebook mode to work in offline\n",
    "> pyo.init_notebook_mode()    \n",
    "> ```\n",
    ">\n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot) But if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_  \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ba222",
   "metadata": {},
   "source": [
    "### 6. Explain why the first expression below can be interpreted as \"the proportion of variation in (outcome) Y explained by the model (i.e. _fitted_model.fittedvalues_)\"; and therefore, why _fitted_model.rsquared_ can be interpreted as a measure of the accuracy of the model; and, therefore what the two _np.corrcoef(...)[0,1]\\*\\*2_ expressions capture in the context of _Simple Linear Regression models_.\n",
    "\n",
    "1. `1-((Y-fitted_model.fittedvalues)**2).sum()/((Y-Y.mean())**2).sum()`\n",
    "2. `fitted_model.rsquared`\n",
    "3. `np.corrcoef(Y,fitted_model.fittedvalues)[0,1]**2`\n",
    "4. `np.corrcoef(Y,x)[0,1]**2`<br><br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _**R-squared** is the \"the proportion of variation in (outcome) $Y$ explained by the model ($\\hat y_i$)\" and is defined as_\n",
    ">\n",
    "> $R^2 = 1 - \\frac{\\sum_{i=1}^n(Y_i-\\hat y)^2}{\\sum_{i=1}^n(Y_i-\\bar Y)^2}$\n",
    ">\n",
    "> _The visuzation provided in the previous problem can be used to consider $(Y_i-\\bar Y)^2$ as the squared distance of the $Y_i$ to their sample average $\\bar Y$ as opposed to the squared **residuals** $(Y_i-\\hat y)^2$ which is the squared distance of the $Y_i$ to their fitted (predicted) values $Y_i$._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot) But if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821fd86",
   "metadata": {},
   "source": [
    "### 7. Indicate a couple of the assumptions of the *Simple Linear Regression* model specification that do not seem compatible with the example data below<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _Hint: What even ARE the assumptions of the  **Simple Linear Regression** model, you ask? Have a look at the mathematical specification and see if what it seems to be assuming._\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot) But if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7fda0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# This data shows the relationship between the amount of fertilizer used and crop yield\n",
    "data = {'Amount of Fertilizer (kg) (x)': [1, 1.2, 1.4, 1.6, 1.8, 2, 2.2, 2.4, 2.6, \n",
    "                                          2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, \n",
    "                                          4.6, 4.8, 5, 5.2, 5.4, 5.6, 5.8, 6, 6.2, \n",
    "                                          6.4, 6.6, 6.8, 7, 7.2, 7.4, 7.6, 7.8, 8, \n",
    "                                          8.2, 8.4, 8.6, 8.8,9, 9.2, 9.4, 9.6],\n",
    "        'Crop Yield (tons) (Y)': [18.7, 16.9, 16.1, 13.4, 48.4, 51.9, 31.8, 51.3, \n",
    "                                  63.9, 50.6, 58.7, 82.4, 66.7, 81.2, 96.5, 112.2, \n",
    "                                  132.5, 119.8, 127.7, 136.3, 148.5, 169.4, 177.9, \n",
    "                                  186.7, 198.1, 215.7, 230.7, 250.4, 258. , 267.8, \n",
    "                                  320.4, 302. , 307.2, 331.5, 375.3, 403.4, 393.5,\n",
    "                                  434.9, 431.9, 451.1, 491.2, 546.8, 546.4, 558.9]}\n",
    "df = pd.DataFrame(data)\n",
    "fig1 = px.scatter(df, x='Amount of Fertilizer (kg) (x)', y='Crop Yield (tons) (Y)',\n",
    "                  trendline='ols', title='Crop Yield vs. Amount of Fertilizer')\n",
    "\n",
    "# Perform linear regression using scipy.stats\n",
    "slope, intercept, r_value, p_value, std_err = \\\n",
    "    stats.linregress(df['Amount of Fertilizer (kg) (x)'], df['Crop Yield (tons) (Y)'])\n",
    "# Predict the values and calculate residuals\n",
    "y_hat = intercept + slope * df['Amount of Fertilizer (kg) (x)']\n",
    "residuals = df['Crop Yield (tons) (Y)'] - y_hat\n",
    "df['Residuals'] = residuals\n",
    "fig2 = px.histogram(df, x='Residuals', nbins=10, title='Histogram of Residuals',\n",
    "                    labels={'Residuals': 'Residuals'})\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    subplot_titles=('Crop Yield vs. Amount of Fertilizer', \n",
    "                                    'Histogram of Residuals'))\n",
    "for trace in fig1.data:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "for trace in fig2.data:\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "fig.update_layout(title='Scatter Plot and Histogram of Residuals',\n",
    "    xaxis_title='Amount of Fertilizer (kg)', yaxis_title='Crop Yield (tons)',\n",
    "    xaxis2_title='Residuals', yaxis2_title='Frequency', showlegend=False)\n",
    "\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977ba64",
   "metadata": {},
   "source": [
    "## \"Week of Nov04\" HW [due prior to the Nov08 TUT]\n",
    "\n",
    "_**In place of the \"Data Analysis Assignment\" format we introduced for the previous weeks' HW, the remaining questions will be a collection of exercises based around the following data**_\n",
    "\n",
    "> The details of the \"LOWESS Trendline\" shown below are not a part of the intended scope of the activities here, but it is included since it is suggestive of the questions we will consider and address here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29449cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# The \"Classic\" Old Faithful Geyser dataset: ask a ChatBot for more details if desired\n",
    "old_faithful = sns.load_dataset('geyser')\n",
    "\n",
    "# Create a scatter plot with a Simple Linear Regression trendline\n",
    "fig = px.scatter(old_faithful, x='waiting', y='duration', \n",
    "                 title=\"Old Faithful Geyser Eruptions\", \n",
    "                 trendline='ols')#'lowess'\n",
    "\n",
    "# Add a smoothed LOWESS Trendline to the scatter plot\n",
    "lowess = sm.nonparametric.lowess  # Adjust 'frac' to change \"smoothness bandwidth\"\n",
    "smoothed = lowess(old_faithful['duration'], old_faithful['waiting'], frac=0.25)  \n",
    "smoothed_df = pd.DataFrame(smoothed, columns=['waiting', 'smoothed_duration'])\n",
    "fig.add_scatter(x=smoothed_df['waiting'], y=smoothed_df['smoothed_duration'], \n",
    "                mode='lines', name='LOWESS Trendline')\n",
    "\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db85ec",
   "metadata": {},
   "source": [
    "### 8. Specify a *null hypothesis* of \"no linear association (on average)\" in terms of the relevant *parameter* of the *Simple Linear Regression* model, and use the code below to characterize the evidence in the data relative to the *null hypothesis* and interpret your subsequent beliefs regarding the Old Faithful Geyser dataset.<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _Remember that **Hypothesis Testing** is not a \"mathematical proof\"_\n",
    ">\n",
    "> - _We do not prove $H_0$ false, we instead give evidence against the $H_0$: \"We reject the null hypothesis with a p-value of XYZ, meaning we have ABC evidence against the null hypothesis\"_\n",
    "> - _We do not prove $H_0$ is true, we instead do not have evidence to reject $H_0$: \"We fail to reject the null hypothesis with a p-value of XYZ\"_\n",
    "\n",
    "|p-value|Evidence|\n",
    "|-|-|\n",
    "|$$p > 0.1$$|No evidence against the null hypothesis|\n",
    "|$$0.1 \\ge p > 0.05$$|Weak evidence against the null hypothesis|\n",
    "|$$0.05 \\ge p > 0.01$$|Moderate evidence against the null hypothesis|\n",
    "|$$0.01 \\ge p > 0.001$$|Strong evidence against the null hypothesis|\n",
    "|$$0.001 \\ge p$$|Very strong evidence against the null hypothesis|\n",
    "\n",
    "</details>    \n",
    "\n",
    "> ```python\n",
    "> import seaborn as sns\n",
    "> import statsmodels.formula.api as smf\n",
    ">\n",
    "> # The \"Classic\" Old Faithful Geyser dataset\n",
    "> old_faithful = sns.load_dataset('geyser')\n",
    "> \n",
    "> linear_for_specification = 'duration ~ waiting'\n",
    "> model = smf.ols(linear_for_specification, data=old_faithful)\n",
    "> fitted_model = model.fit()\n",
    "> fitted_model.summary()\n",
    "> ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd719fef",
   "metadata": {},
   "source": [
    "### 9. As seen in the introductory figure above, if the delay of the geyser eruption since the previous geyser eruption exceeds approximately 63 minutes, there is a notable increase in the duration of the geyser eruption itself. In the figure below we therefore restrict the dataset to only short wait times. Within the context of only short wait times, is there evidence in the data for a relationship between duration and wait time in the same manner as in the full data set? Using the following code, characterize the evidence against the *null hypothesis* in the context of short wait times which are less than  *short_wait_limit* values of *62*, *64*, *66*.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "short_wait_limit = 62 # 64 # 66 #\n",
    "short_wait = old_faithful.waiting < short_wait_limit\n",
    "\n",
    "print(smf.ols('duration ~ waiting', data=old_faithful[short_wait]).fit().summary().tables[1])\n",
    "\n",
    "# Create a scatter plot with a linear regression trendline\n",
    "fig = px.scatter(old_faithful[short_wait], x='waiting', y='duration', \n",
    "                 title=\"Old Faithful Geyser Eruptions for short wait times (<\"+str(short_wait_limit)+\")\", \n",
    "                 trendline='ols')\n",
    "\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009bfa79",
   "metadata": {},
   "source": [
    "### 10. Let's now consider just the (*n=160*) long wait times (as specified in the code below), and write code to do the following:\n",
    "\n",
    "1. create fitted **Simple Linear Regression** models for **boostrap samples** and collect and visualize the **bootstrapped sampling distribution** of the **fitted slope coefficients** of the fitted models;  \n",
    "\n",
    "\n",
    "2. **simulate** samples (of size `n=160`) from a **Simple Linear Regression** model that uses $\\beta_0 = 1.65$, $\\beta_1 = 0$, $\\sigma = 0.37$ along with the values of `waiting` for $x$ to create **simuations** of $Y$ and use these collect and visualize the **sampling distribution** of the **fitted slope coefficient** under a **null hypothesis** assumption of \"no linear association (on average)\"; then,  \n",
    "\n",
    "\n",
    "3. report if $0$ is contained within a 95\\% **bootstrapped confidence interval**; and if the **simulated p-value** matches `smf.ols('duration ~ waiting', data=old_faithful[long_wait]).fit().summary().tables[1]`?<br><br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _You'll need to create `for` loops to repeatedly create fitted **Simple Linear Regression** models using different samples, collecting the **fitted slope coeffient** created in each `for` loop \"step\" in order to visualize the **simulated sampling distributions**_\n",
    "> \n",
    "> - _A **bootstrapped sample** of the \"long wait times\" dataset can be created with `old_faithful[long_wait].sample(n=long_wait.sum(), replace=True)`_\n",
    ">\n",
    ">\n",
    "> - _A **simulated** version of the \"long wait times under a null hypothesis assumption of **no linear association (on average)**\" dataset can be created by first creating `old_faithful_simulation = old_faithful[long_wait].copy()` and then assigning the **simulated** it values with `old_faithful_simulation['duration'] = 1.65 + 0*old_faithful_simulation.waiting + stats.norm(loc=0, scale=0.37).rvs(size=long_wait.sum())`_ \n",
    ">\n",
    ">  _The values $\\beta_0 = 1.65$ and $\\sigma = 0.37$ are chosen to match what is actually observed in the data, while $\\beta_1 = 0$ is chosen to reflect a **null hypothesis** assumption of \"no linear assocaition (on average)\"; and, make sure that you understand why it is that_\n",
    ">\n",
    ">\n",
    "> - _if `bootstrapped_slope_coefficients` is the `np.array` of your **bootstrapped slope coefficients** then `np.quantile(bootstrapped_slope_coefficients, [0.025, 0.975])` is a 95\\% **bootstrapped confidence interval**_\n",
    "> \n",
    ">\n",
    "> - _if `simulated_slope_coefficients` is the `np.array` of your **fitted slope coefficients** **simulated** under a **null hypothesis** \"no linear association (on average)\" then `(np.abs(simulated_slope_coefficients) >= smf.ols('duration ~ waiting', data=old_faithful[long_wait]).fit().params[1]).mean()` is the **p-value** for the **simulated** **simulated sampling distribution of the slope coeficients** under a **null hypothesis** \"no linear association (on average)\"_\n",
    "\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c246ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "long_wait_limit = 71\n",
    "long_wait = old_faithful.waiting > long_wait_limit\n",
    "\n",
    "print(smf.ols('duration ~ waiting', data=old_faithful[long_wait]).fit().summary().tables[1])\n",
    "\n",
    "# Create a scatter plot with a linear regression trendline\n",
    "fig = px.scatter(old_faithful[long_wait], x='waiting', y='duration', \n",
    "                 title=\"Old Faithful Geyser Eruptions for short wait times (>\"+str(long_wait_limit)+\")\", \n",
    "                 trendline='ols')\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1163d93",
   "metadata": {},
   "source": [
    "### 11. Since we've considered wait times of around <64  \"short\" and wait times of >71 \"long\", let's instead just divide the data and insead call wait times of <68 \"short\" and otherwise just call them \"long\". Consider the *Simple Linear Regression* model specification using an *indicator variable* of the wait time length<br>\n",
    "\n",
    "$$\\large Y_i = \\beta_{\\text{intercept}} + 1_{[\\text{\"long\"}]}(\\text{k_i})\\beta_{\\text{contrast}} + \\epsilon_i \\quad \\text{ where } \\quad \\epsilon_i \\sim \\mathcal N\\left(0, \\sigma\\right)$$\n",
    "\n",
    "### where we use $k_i$ (rather than $x_i$) (to refer to the \"kind\" or \"katagory\" or \"kontrast\") column (that you may have noticed was already a part) of the original dataset; and, explain the \"big picture\" differences between this model specification and the previously considered model specifications<br>\n",
    "\n",
    "1. `smf.ols('duration ~ waiting', data=old_faithful)`\n",
    "2. `smf.ols('duration ~ waiting', data=old_faithful[short_wait])`\n",
    "3. `smf.ols('duration ~ waiting', data=old_faithful[long_wait])`\n",
    "\n",
    "### and report the evidence against a *null hypothesis* of \"no difference between groups \"on average\") for the new *indicator variable* based model<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(smf.ols('duration ~ C(kind, Treatment(reference=\"short\"))', data=old_faithful).fit().summary().tables[1])\n",
    "\n",
    "fig = px.box(old_faithful, x='kind', y='duration', \n",
    "             title='duration ~ kind',\n",
    "             category_orders={'kind': ['short', 'long']})\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee975f",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot) But if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f288839",
   "metadata": {},
   "source": [
    "### 12. Identify which of the histograms suggests the plausibility of the assumption that the distribution of *error* terms is normal for each of the models, and explain why the other three do not support this assumption.\n",
    "\n",
    "> Hint: Question 5 of the *Communication Activity #2* of the Oct25 TUT (addressing an *omitted* section of the TUT) discusses how the assumption in *Simple Linear Regression* that the *error* terms $\\epsilon_i \\sim \\mathcal N\\left(0, \\sigma\\right)$ is diagnostically assessed by evaluating distributional shape of the *residuals* $\\text{e}_i = \\hat \\epsilon_i = Y_i - \\hat y_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e3a5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "model_residuals = {\n",
    "    '<br>Model 1:<br>All Data using slope': smf.ols('duration ~ waiting', data=old_faithful).fit().resid,\n",
    "    '<br>Model 2:<br>Short Wait Data': smf.ols('duration ~ waiting', data=old_faithful[short_wait]).fit().resid,\n",
    "    '<br>Model 3:<br>Long Wait Data': smf.ols('duration ~ waiting', data=old_faithful[long_wait]).fit().resid,\n",
    "    '<br>Model 4:<br>All Data using indicator': smf.ols('duration ~ C(kind, Treatment(reference=\"short\"))', data=old_faithful).fit().resid\n",
    "}\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=list(model_residuals.keys()))\n",
    "for i, (title, resid) in enumerate(model_residuals.items()):\n",
    "\n",
    "    if i == 1:  # Apply different bins only to the second histogram (index 1)\n",
    "        bin_size = dict(start=-1.9, end=1.9, size=0.2)\n",
    "    else:\n",
    "        bin_size = dict(start=-1.95, end=1.95, size=0.3)\n",
    "\n",
    "    fig.add_trace(go.Histogram(x=resid, name=title, xbins=bin_size, histnorm='probability density'), \n",
    "                  row=int(i/2)+1, col=(i%2)+1)\n",
    "    fig.update_xaxes(title_text=\"n=\"+str(len(resid)), row=int(i/2)+1, col=(i%2)+1)    \n",
    "    \n",
    "    normal_range = np.arange(-3*resid.std(),3*resid.std(),0.01)\n",
    "    fig.add_trace(go.Scatter(x=normal_range, mode='lines', opacity=0.5,\n",
    "                             y=stats.norm(loc=0, scale=resid.std()).pdf(normal_range),\n",
    "                             line=dict(color='black', dash='dot', width=2),\n",
    "                             name='Normal Distribution<br>(99.7% of its area)'), \n",
    "                  row=int(i/2)+1, col=(i%2)+1)\n",
    "    \n",
    "fig.update_layout(title_text='Histograms of Residuals from Different Models')\n",
    "fig.update_xaxes(range=[-2,2])\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7413283",
   "metadata": {},
   "source": [
    "### 13. The \"short\" and \"long\" wait times are not \"before and after\" measurements so there are not natural pairs on which to base differences on which to do a \"one sample\" (paired differences) *hypothesis test*; but, we can do \"two sample\" hypothesis testing using a *permuation test*, or create a 95% *bootstrap confidence interval* for the difference in means of the two populations. \n",
    "\n",
    "### (A) Do a permuation test $\\;H_0: \\mu_{\\text{short}}=\\mu_{\\text{long}} \\; \\text{ no difference in duration between short and long groups}$ by \"shuffling\" the labels\n",
    "### (B) Create a 95% bootstrap confidence interval  by repeatedly bootstrapping within each group and applying *np.quantile(bootstrapped_mean_differences, [0.025, 0.975])* to the collection of differences between the sample means.    \n",
    "### (a) Explain how the sampling approaches work for the two simulations.\n",
    "### (b) Compare and contrast these two methods with the *indicator variable* based model approach used in Question 11, explaining how they're similar and different.<br>\n",
    "    \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _You'll need to create `for` loops for repeated (shuffling simulation) **permutation** and (subgroup) **bootstrapping**, where_\n",
    ">\n",
    "> - _\"shuffling\" for **permutation testing** is done like this `old_faithful.assign(kind_shuffled=old_faithful['kind'].sample(n=len(old_faithful), replace=False).values)#.groupby('kind').size()`; then, the **mean difference statistic** is then calculated using `.groupby('kind_shuffled')['duration'].mean().iloc[::-1].diff().values[1]` (so the **observed statistic** is `old_faithful.groupby('kind')['duration'].mean().iloc[::-1].diff().values[1]`_\n",
    "> \n",
    ">\n",
    "> - _\"two sample\" **bootstrapping** is done like this `old_faithful.groupby('kind').apply(lambda x: x.sample(n=len(x), replace=True)).reset_index(drop=True)#.groupby('kind').size()`; then, the **bootstrapped mean difference statistic** is then calculated using `.groupby('kind')['duration'].mean().iloc[::-1].diff().values[1]` (like the **observed statistic** except this is applied to the **bootstrapped** resampling of `old_faithful`)_\n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot) But if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe02c1",
   "metadata": {},
   "source": [
    "### 14. Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    ">  _Here is the link of [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) in case it gets lost among all the information you need to keep track of_  : )\n",
    ">\n",
    "> _Just answering \"Yes\" or \"No\" or \"Somewhat\" or \"Mostly\" or whatever here is fine as this question isn't a part of the rubric; but, the midterm and final exams may ask questions that are based on the tutorial and lecture materials; and, your own skills will be limited by your familiarity with these materials (which will determine your ability to actually do actual things effectively with these skills... like the course project...)_\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0639e8",
   "metadata": {},
   "source": [
    "## Recommended Additional Useful Activities [Optional]\n",
    "\n",
    "The \"Ethical Profesionalism Considerations\" and \"Current Course Project Capability Level\" sections below **are not a part of the required homework assignment**; rather, they are regular weekly guides covering (a) relevant considerations regarding professional and ethical conduct, and (b) the analysis steps for the STA130 course project that are feasible at the current stage of the course \n",
    "\n",
    "<br>\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Ethical Professionalism Considerations</u></summary>\n",
    "\n",
    "### Ethical Professionalism Considerations\n",
    "    \n",
    "The TUT and HW both addressed some of the assumptions used in **Simple Linear Regression**. The **p-values** provided by `statsmodels` via `smf.ols(...).fit()` depend on these assumptions, so if they are not (at least approximately) correct, the **p-values** (and any subsequent claims regarding the \"evidience against\" the **null hypothesis**) are not reliable. In light of this consideration, describe how you could diagnostically check the first three assumptions (given below) when using analyses based on **Simple Linear regression** model. From an Ethical and Professional perspective, do you think doing diagnostic checks on the assumptions of a **Simple Linear regression** model is something you can and should do whenever you're doing this kind of analysis? \n",
    "            \n",
    "> The first three assumptions associated with the **Simple Linear regression** model are that\n",
    "> \n",
    "> - the $\\epsilon_i$ **errors** (sometimes referred to as the **noise**) are **normally distributed**\n",
    "> - the $\\epsilon_i$ **errors** are **homoscedastic** (so their distributional variance $\\sigma^2$ does not change as a function of $x_i$)\n",
    "> - the linear form is [at least reasonably approximately] \"true\" (in the sense that the above two remain [at least reasonably approximately] \"true\") so that then behavior of the $Y_i$ **outcomes** are represented/determined on average by the **linear equation**)<br>\n",
    "> \n",
    ">    and there are additional assumptions; but, a deeper reflection on these is \"beyond the scope\" of STA130; nonetheless, they are that<br><br>\n",
    "> - the $x_i$ **predictor variable** is **measured without error**\n",
    "> - and the $\\epsilon_i$ **errors** are **statistically independent** (so their values do not depend on each other)\n",
    "> - and the $\\epsilon_i$ **errors** are **unbiased** relative to the **expected value** of **outcome** $E[Y_i|x_i]=\\beta_0 + \\beta_1x_i$ (which is equivalently stated by saying that the mean of the **error distribution** is $0$, or again equivalently, that the **expected value** of the **errors** $E[\\epsilon_i] = 0$)\n",
    "    \n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Current Course Project Capability Level</u></summary>\n",
    "\n",
    "**Remember to abide by the [data use agreement](https://static1.squarespace.com/static/60283c2e174c122f8ebe0f39/t/6239c284d610f76fed5a2e69/1647952517436/Data+Use+Agreement+for+the+Canadian+Social+Connection+Survey.pdf) at all times.**\n",
    "\n",
    "Information about the course project is available on the course github repo [here](https://github.com/pointOfive/stat130chat130/tree/main/CP), including a draft [course project specfication](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F23_course_project_specification.ipynb) (subject to change). \n",
    "- The Week 01 HW introduced [STA130F24_CourseProject.ipynb](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F24_CourseProject.ipynb), and the [available variables](https://drive.google.com/file/d/1ISVymGn-WR1lcRs4psIym2N3or5onNBi/view). \n",
    "- Please do not download the [data](https://drive.google.com/file/d/1mbUQlMTrNYA7Ly5eImVRBn16Ehy9Lggo/view) accessible at the bottom of the [CSCS](https://casch.org/cscs) webpage (or the course github repo) multiple times.\n",
    "    \n",
    "> ### NEW DEVELOPMENT<br>New Abilities Achieved and New Levels Unlocked!!!    \n",
    "> **As noted, the Week 01 HW introduced the [STA130F24_CourseProject.ipynb](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F24_CourseProject.ipynb) notebook.** _And there it instructed students to explore the notebook through the first 16 cells of the notebook._ The following cell in that notebook (there marked as \"run cell 17\") is preceded by an introductory section titled, \"**Now for some comparisons...**\", _**and all material from that point on provides an example to allow you to start applying what you're learning about Hypothesis Testing to the CSCS data**_ **using a paired samples (\"one sample\") framework.**\n",
    ">\n",
    "> **NOW, HOWEVER, YOU CAN DO MORE.** \n",
    "> - _**Now you can do \"two sample\" hypothesis testing without the need for paired samples.**_ All you need are two groups.\n",
    "> - _**And now you can do simple linear regression modeling.**_ All you need are two columns.\n",
    "\n",
    "### Current Course Project Capability Level\n",
    "\n",
    "At this point in the course you should be able to do a **Simple Linear Regression** analysis for data from the Canadian Social Connection Survey data\n",
    "    \n",
    "1. Create and test a **null hypothesis** of no linear association \"on average\" for a couple of columns of interest in the Canadian Social Connection Survey data using **Simple Linear Regression**\n",
    "\n",
    "2. Use the **residuals** of a fitted **Simple Linear Regression** model to diagnostically assess some of the assumptions of the analysis\n",
    "\n",
    "3. Use an **indicator variable** based **Simple Linear Regression** model to compare two groups from the Canadian Social Connection Survey data\n",
    "\n",
    "4. Compare and contrast the results of an **indicator variable** based **Simple Linear Regression** model to analyses based on a **permutation test** and a **bootstrapped confidence interval**   \n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a0b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
